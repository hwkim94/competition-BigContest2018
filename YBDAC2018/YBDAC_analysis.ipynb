{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import cufflinks as cf\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "#import nltk\n",
    "from konlpy.tag import Twitter\n",
    "#import geocoder\n",
    "from py_hanspell.hanspell import spell_checker\n",
    "\n",
    "cf.go_offline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install konlpy\n",
    "#!pip install py-hanspell\n",
    "#!pip install nltk\n",
    "#!pip install geocoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "file = pd.read_csv(\"data/preprocessed_teo.csv\", encoding=\"cp949\")\n",
    "file.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br> <br></br> <br></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_lst = [\"문의일자\", \"간부여부\", \"입찰참가자격\", \"입찰마감일\", \"파일명 #1\", \"파일명 #2\", \"사업기간\", \"결과구분\",\n",
    "            \"계약기간\", \"계약액\", \"경쟁사\", \"최종선정사\", \"결과보고\", \"link\", \"기업명\", \"기업개요 및 비전\", \"영업이익\"]\n",
    "file2 = file.drop(drop_lst, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file2.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br> <br></br> <br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_money(text) :\n",
    "    lst = text.replace(\"원\",\"\").replace(\",\",\"\").split()\n",
    "    result = 0\n",
    "\n",
    "    for idx in lst :\n",
    "        if \"조\" in idx :\n",
    "            temp = eval(idx.replace(\"조\",\"*10000\"))\n",
    "            \n",
    "        if \"억\" in idx :\n",
    "            temp = eval(idx.replace(\"억\",\"*1\"))\n",
    "            \n",
    "        if \"만\" in idx :\n",
    "            temp = eval(idx.replace(\"만\",\"*0.0001\"))\n",
    "            \n",
    "        result += temp\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def money(x) :\n",
    "    temp_lst = list(x)[-25:]\n",
    "    cnt = 0\n",
    "    total = 0\n",
    "    \n",
    "    for idx, company in enumerate(temp_lst) :\n",
    "        if  company == 1 :\n",
    "            total += eval(\"temp\" +str(idx+1))\n",
    "            cnt+=1\n",
    "        \n",
    "    else :\n",
    "        for idx in range(len(temp_lst)) :\n",
    "            total += eval(\"temp\" +str(idx+1))\n",
    "            cnt+=1\n",
    "        \n",
    "    return total/cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 고객사\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문의 개요\n",
    "# 한국말만 남겨주기\n",
    "\n",
    "file2[\"문의개요_regex\"] = file2[\"문의개요\"].map(lambda x : re.sub(r'[^가-힣 ]', '', x) if type(x)==str else \"\")\n",
    "file2.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 접수경로\n",
    "# 정리\n",
    "# 카테고리로 전환 -> onehot\n",
    "\n",
    "file2[\"접수경로\"] = file2[\"접수경로\"].map(lambda x : \"현\" if x==\"now\" else \"전\" if x==\"past\" else \"소개\" if x not in [\"전\", \"현\", \"공고\", \"대표연락\", \"변\", \"소개\", \"신문\"] else x)\n",
    "file2 = pd.concat([file2, pd.get_dummies(file2[\"접수경로\"])], axis=1).drop(\"접수경로\", axis=1)\n",
    "file2.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 채널\n",
    "# 카테고리로 전환 -> onehot\n",
    "\n",
    "file2 = pd.concat([file2, pd.get_dummies(file2[\"채널\"])], axis=1).drop(\"채널\", axis=1)\n",
    "file2.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 접수내용\n",
    "# 한국말만 남겨주기\n",
    "\n",
    "file2[\"접수내용_regex\"] = file2[\"접수내용\"].map(lambda x : re.sub(r'[^가-힣 ]', '', x) if type(x)==str else \"\")\n",
    "file2[\"접수내용\"] = file2[\"접수내용\"].map(lambda x : x.replace(\"-\", \"\").split(\"\\n\") if type(x)==str else \"\")\n",
    "file2.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Budget\n",
    "# NaN 값을 mean으로 처리 해주기 및 정규화\n",
    "\n",
    "file2[\"Budget\"] = file2[\"Budget\"].fillna(file2[\"Budget\"].dropna().mean())\n",
    "file2[\"Budget\"] = (file2[\"Budget\"] - file2[\"Budget\"].mean())/file2[\"Budget\"].std()\n",
    "file2.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bid \n",
    "# 카테고리로 전환 -> onehot\n",
    "\n",
    "file2 = pd.concat([file2, pd.get_dummies(file2[\"Bid\"])], axis=1).drop(\"Bid\", axis=1).rename(columns = {\"기타\" : \"Bid_기타\"})\n",
    "file2.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 진행\n",
    "# 카테고리로 전환 -> onehot\n",
    "\n",
    "file2 = pd.concat([file2, pd.get_dummies(file2[\"진행\"])], axis=1).drop(\"진행\", axis=1)\n",
    "file2.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과\n",
    "# 카테고리로 전환 -> onehot\n",
    "\n",
    "file2 = pd.concat([file2, pd.get_dummies(file2[\"결과\"])], axis=1).drop(\"결과\", axis=1)\n",
    "file2.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 소속그룹\n",
    "# 0,1 로 나누기\n",
    "\n",
    "file2[\"소속그룹\"] = file2[\"소속그룹\"].map(lambda x : 0 if type(x) == float else 1)\n",
    "file2.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 업종\n",
    "# 카테고리로 전환 -> onehot\n",
    "\n",
    "file2 = pd.concat([file2, pd.get_dummies(file2[\"업종\"])], axis=1).drop(\"업종\", axis=1)\n",
    "file2.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사업내용\n",
    "# 한국말만 남겨주기\n",
    "\n",
    "file2[\"사업내용_regex\"] = file2[\"사업내용\"].map(lambda x : re.sub(r'[^가-힣 ]', '', x) if type(x)==str else \"\")\n",
    "file2.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기업형태\n",
    "# 카테고리로 전환 \n",
    "\n",
    "file2[\"기업형태\"] = file2[\"기업형태\"].map(lambda x : x.split(\", \") if type(x) == str else [])\n",
    "\n",
    "v = file2[\"기업형태\"].values\n",
    "l = [len(x) for x in v.tolist()]\n",
    "f, u = pd.factorize(np.concatenate(v))\n",
    "n, m = len(v), u.size\n",
    "i = np.arange(n).repeat(l)\n",
    "\n",
    "dummies = pd.DataFrame(\n",
    "    np.bincount(i * m + f, minlength=n * m).reshape(n, m),\n",
    "    file2.index, u\n",
    ")\n",
    "\n",
    "file2 = pd.concat([file2, dummies], axis=1).drop(\"기업형태\", axis=1)\n",
    "file2.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기업주소\n",
    "# 좌표로 전환\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 자본금\n",
    "# regex\n",
    "# NaN 값을 기업유형별 평균으로 처리 해주기 및 정규화\n",
    "\n",
    "temp1 = file2[file2[\"기타\"] == 1][\"자본금\"].dropna().map(lambda x : change_money(x)).mean()\n",
    "temp2 = file2[file2[\"공사/공기업\"] == 1][\"자본금\"].dropna().map(lambda x : change_money(x)).mean()\n",
    "temp3 = file2[file2[\"1000대기업\"] == 1][\"자본금\"].dropna().map(lambda x : change_money(x)).mean()\n",
    "temp4 = file2[file2[\"재단법인\"] == 1][\"자본금\"].dropna().map(lambda x : change_money(x)).mean()\n",
    "temp5 = file2[file2[\"비영리단체/협회\"] == 1][\"자본금\"].dropna().map(lambda x : change_money(x)).mean()\n",
    "temp6 = file2[file2[\"대기업\"] == 1][\"자본금\"].dropna().map(lambda x : change_money(x)).mean()\n",
    "temp7 = file2[file2[\"외부감사법인\"] == 1][\"자본금\"].dropna().map(lambda x : change_money(x)).mean()\n",
    "temp8 = file2[file2[\"중견기업\"] == 1][\"자본금\"].dropna().map(lambda x : change_money(x)).mean()\n",
    "temp9 = file2[file2[\"중소기업\"] == 1][\"자본금\"].dropna().map(lambda x : change_money(x)).mean()\n",
    "temp10 = file2[file2[\"외국인 투자기업\"] == 1][\"자본금\"].dropna().map(lambda x : change_money(x)).mean()\n",
    "temp11 = file2[file2[\"주식회사\"] == 1][\"자본금\"].dropna().map(lambda x : change_money(x)).mean()\n",
    "temp12 = file2[file2[\"학교/교육기관\"] == 1][\"자본금\"].dropna().map(lambda x : change_money(x)).mean()\n",
    "temp13 = file2[file2[\"수출입 기업\"] == 1][\"자본금\"].dropna().map(lambda x : change_money(x)).mean()\n",
    "temp14 = file2[file2[\"병원/의료기관\"] == 1][\"자본금\"].dropna().map(lambda x : change_money(x)).mean()\n",
    "temp15 = file2[file2[\"사단법인\"] == 1][\"자본금\"].dropna().map(lambda x : change_money(x)).mean()\n",
    "temp16 = file2[file2[\"코스피\"] == 1][\"자본금\"].dropna().map(lambda x : change_money(x)).mean()\n",
    "temp17 = file2[file2[\"연구소\"] == 1][\"자본금\"].dropna().map(lambda x : change_money(x)).mean()\n",
    "temp18 = file2[file2[\"코스닥\"] == 1][\"자본금\"].dropna().map(lambda x : change_money(x)).mean()\n",
    "temp19 = file2[file2[\"외국 법인기업\"] == 1][\"자본금\"].dropna().map(lambda x : change_money(x)).mean()\n",
    "temp20 = file2[file2[\"협동조합\"] == 1][\"자본금\"].dropna().map(lambda x : change_money(x)).mean()\n",
    "temp21 = file2[file2[\"금융기관\"] == 1][\"자본금\"].dropna().map(lambda x : change_money(x)).mean()\n",
    "temp22 = file2[file2[\"병역특례 인증업체\"] == 1][\"자본금\"].dropna().map(lambda x : change_money(x)).mean()\n",
    "temp23 = file2[file2[\"유한회사\"] == 1][\"자본금\"].dropna().map(lambda x : change_money(x)).mean()\n",
    "temp24 = file2[file2[\"코넥스\"] == 1][\"자본금\"].dropna().map(lambda x : change_money(x)).mean()\n",
    "temp25 = file2[file2[\"사회복지기관\"] == 1][\"자본금\"].dropna().map(lambda x : change_money(x)).mean()\n",
    "\n",
    "file2[\"자본금\"] = file2[\"자본금\"].map(lambda x : change_money(x) if type(x) == str else \"\")\n",
    "file2[\"자본금\"] = file2.apply(lambda x : money(x) if x[\"자본금\"]==\"\" else x[\"자본금\"], axis=1)\n",
    "file2[\"자본금\"] = (file2[\"자본금\"] - file2[\"자본금\"].mean())/file2[\"자본금\"].std()\n",
    "file2.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 매출액\n",
    "# regex\n",
    "# NaN 값을 기업유형별 평균으로 처리 해주기 및 정규화\n",
    "\n",
    "temp1 = file2[file2[\"기타\"] == 1][\"매출액\"].dropna().map(lambda x : change_money(x)).mean()\n",
    "temp2 = file2[file2[\"공사/공기업\"] == 1][\"매출액\"].dropna().map(lambda x : change_money(x)).mean()\n",
    "temp3 = file2[file2[\"1000대기업\"] == 1][\"매출액\"].dropna().map(lambda x : change_money(x)).mean()\n",
    "temp4 = file2[file2[\"재단법인\"] == 1][\"매출액\"].dropna().map(lambda x : change_money(x)).mean()\n",
    "temp5 = file2[file2[\"비영리단체/협회\"] == 1][\"매출액\"].dropna().map(lambda x : change_money(x)).mean()\n",
    "temp6 = file2[file2[\"대기업\"] == 1][\"매출액\"].dropna().map(lambda x : change_money(x)).mean()\n",
    "temp7 = file2[file2[\"외부감사법인\"] == 1][\"매출액\"].dropna().map(lambda x : change_money(x)).mean()\n",
    "temp8 = file2[file2[\"중견기업\"] == 1][\"매출액\"].dropna().map(lambda x : change_money(x)).mean()\n",
    "temp9 = file2[file2[\"중소기업\"] == 1][\"매출액\"].dropna().map(lambda x : change_money(x)).mean()\n",
    "temp10 = file2[file2[\"외국인 투자기업\"] == 1][\"매출액\"].dropna().map(lambda x : change_money(x)).mean()\n",
    "temp11 = file2[file2[\"주식회사\"] == 1][\"매출액\"].dropna().map(lambda x : change_money(x)).mean()\n",
    "temp12 = file2[file2[\"학교/교육기관\"] == 1][\"매출액\"].dropna().map(lambda x : change_money(x)).mean()\n",
    "temp13 = file2[file2[\"수출입 기업\"] == 1][\"매출액\"].dropna().map(lambda x : change_money(x)).mean()\n",
    "temp14 = file2[file2[\"병원/의료기관\"] == 1][\"매출액\"].dropna().map(lambda x : change_money(x)).mean()\n",
    "temp15 = file2[file2[\"사단법인\"] == 1][\"매출액\"].dropna().map(lambda x : change_money(x)).mean()\n",
    "temp16 = file2[file2[\"코스피\"] == 1][\"매출액\"].dropna().map(lambda x : change_money(x)).mean()\n",
    "temp17 = file2[file2[\"연구소\"] == 1][\"매출액\"].dropna().map(lambda x : change_money(x)).mean()\n",
    "temp18 = file2[file2[\"코스닥\"] == 1][\"매출액\"].dropna().map(lambda x : change_money(x)).mean()\n",
    "temp19 = file2[file2[\"외국 법인기업\"] == 1][\"매출액\"].dropna().map(lambda x : change_money(x)).mean()\n",
    "temp20 = file2[file2[\"협동조합\"] == 1][\"매출액\"].dropna().map(lambda x : change_money(x)).mean()\n",
    "temp21 = file2[file2[\"금융기관\"] == 1][\"매출액\"].dropna().map(lambda x : change_money(x)).mean()\n",
    "temp22 = file2[file2[\"병역특례 인증업체\"] == 1][\"매출액\"].dropna().map(lambda x : change_money(x)).mean()\n",
    "temp23 = file2[file2[\"유한회사\"] == 1][\"매출액\"].dropna().map(lambda x : change_money(x)).mean()\n",
    "temp24 = file2[file2[\"코넥스\"] == 1][\"매출액\"].dropna().map(lambda x : change_money(x)).mean()\n",
    "temp25 = file2[file2[\"사회복지기관\"] == 1][\"매출액\"].dropna().map(lambda x : change_money(x)).mean()\n",
    "\n",
    "file2[\"매출액\"] = file2[\"매출액\"].map(lambda x : change_money(x) if type(x) == str else \"\")\n",
    "file2[\"매출액\"] = file2.apply(lambda x : money(x) if x[\"매출액\"]==\"\" else x[\"매출액\"], axis=1)\n",
    "file2[\"매출액\"] = (file2[\"매출액\"] - file2[\"매출액\"].mean())/file2[\"매출액\"].std()\n",
    "file2.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영업이익\n",
    "# regex\n",
    "# NaN 값을 기업유형별 평균으로 처리 해주기 및 정규화\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br> <br></br> <br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 맞춤법 검사\n",
    "- https://github.com/ssut/py-hanspell\n",
    "- https://code-examples.net/ko/q/d486db\n",
    "- https://www.askcompany.kr/vod/crawling/51/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chcking(x) :\n",
    "    global cnt\n",
    "    if tpye(result) == str :\n",
    "        try :\n",
    "            result = spell_checker.check(x).as_dict()[\"checked\"]\n",
    "        except :\n",
    "            result = \"\"\n",
    "            print(cnt)\n",
    "            cnt+=1\n",
    "    else :\n",
    "        result = \"\"\n",
    "        print(cnt)\n",
    "        cnt+=1\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"아버지가방에들어가신다\"\n",
    "result_test = spell_checker.check(test).as_dict()[\"checked\"]\n",
    "print(result_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cnt=0\n",
    "#file2[\"문의개요_regex\"] = file2[\"문의개요_regex\"].map(lambda x : spell_checker.check(x).as_dict()[\"checked\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cnt=0\n",
    "#file2[\"접수내용_regex\"] = file2[\"접수내용_regex\"].map(lambda x : spell_checker.check(x).as_dict()[\"checked\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cnt=0\n",
    "#file2[\"사업내용_regex\"] = file2[\"사업내용_regex\"].map(lambda x : spell_checker.check(x).as_dict()[\"checked\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file2.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br><br></br><br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 명사화\n",
    "- http://dalulu.tistory.com/108\n",
    "- https://datascienceschool.net/view-notebook/70ce46db4ced4a999c6ec349df0f4eb0/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter = Twitter()\n",
    "\n",
    "def noun(line, twitter) :\n",
    "    return twitter.nouns(line)\n",
    "\n",
    "def pos_tagging(line, tweeter) :\n",
    "    return twitter.pos(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file2[\"문의개요_noun\"] = file2[\"문의개요_regex\"].map(lambda x : noun(x,twitter))\n",
    "file2[\"접수내용_noun\"] = file2[\"접수내용_regex\"].map(lambda x : noun(x,twitter))\n",
    "file2[\"사업내용_noun\"] = file2[\"사업내용_regex\"].map(lambda x : noun(x,twitter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br><br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec\n",
    "- https://sites.google.com/site/rmyeid/projects/polyglot\n",
    "- https://github.com/Kyubyong/wordvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec(lst, dic) :\n",
    "    result = []\n",
    "    \n",
    "    for noun in lst :\n",
    "        if noun in dic :\n",
    "            result.append((noun, dic[noun]))\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "a = pd.read_csv(\"word_embedding/ko.csv\", header=None)\n",
    "b = list(map(lambda x : x.split(),list(a[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = []\n",
    "\n",
    "for idx in b :\n",
    "    for idx2 in idx :\n",
    "        lst.append(idx2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = {}\n",
    "temp = []\n",
    "previous = \"\"\n",
    "word = \"\"\n",
    "check = False\n",
    "\n",
    "for idx, elts in enumerate(lst) : \n",
    "    if \"[\" in elts :\n",
    "        word = previous\n",
    "        elts = elts.replace(\"[\", \"\").strip()\n",
    "        check=True\n",
    "        \n",
    "        if elts != \"\" :\n",
    "            temp.append(float(elts))\n",
    "        \n",
    "        previous = elts\n",
    "        continue\n",
    "    \n",
    "    if \"]\" in elts :\n",
    "        elts = elts.replace(\"]\", \"\").strip()\n",
    "        check=False\n",
    "        \n",
    "        if elts != \"\" :\n",
    "            temp.append(float(elts))\n",
    "        \n",
    "        dic[word] = temp\n",
    "        temp=[]\n",
    "        \n",
    "        previous = elts\n",
    "        continue\n",
    "    \n",
    "    if check :\n",
    "        temp.append(float(elts))\n",
    "        \n",
    "    previous = elts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(dic.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "length = []\n",
    "for v in list(dic.values()) :\n",
    "    length.append(len(v))\n",
    "    \n",
    "length = set(length)\n",
    "print(length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec 전환\n",
    "file2[\"문의개요_vector\"] = file2[\"문의개요_noun\"].map(lambda x : word2vec(x,dic))\n",
    "file2[\"접수내용_vector\"] = file2[\"접수내용_noun\"].map(lambda x : word2vec(x,dic))\n",
    "file2[\"사업내용_vector\"] = file2[\"사업내용_noun\"].map(lambda x : word2vec(x,dic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file2.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br><br></br><br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 키워드 추출(TF-IDF)\n",
    "- http://aidev.co.kr/nlp/2209"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(lst) :\n",
    "    try :\n",
    "        doc = \" \".join(lst)\n",
    "    except :\n",
    "        return []\n",
    "\n",
    "    nline = []\n",
    "    for line in lst :\n",
    "        nline.append(noun(line, twitter))\n",
    "    \n",
    "    nline2 = []\n",
    "    for line in nline :\n",
    "        for n in line :\n",
    "            nline2.append(n)\n",
    "            \n",
    "    result = []\n",
    "    for n in nline2 :\n",
    "        N = len(lst)\n",
    "        tf = doc.count(n)\n",
    "        idf = 0\n",
    "        \n",
    "        for line in lst :\n",
    "            if n in line :\n",
    "                idf += 1\n",
    "\n",
    "        result.append([n, tf*np.log(N/idf)])\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf-idf 계산\n",
    "file2[\"접수내용_importance\"] = file2[\"접수내용\"].map(lambda x : tf_idf(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file2.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br><br></br><br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rep2vec (report 2 vec)\n",
    "- word2vec의 가중평균 ver\n",
    "- TF-IDF와 word2vec의 가중평균 ver\n",
    "- TF-IDF와 word2vec의 가중평균 + 그 외의 정보 ver\n",
    "- TF-IDF와 word2vec의 가중평균 + ML을 통한 가중치를 적용한 그 외의 정보 ver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file3 = file2.copy().drop([\"문의개요\", \"접수내용\", \"사업내용\", \"기업주소\", \"문의개요_regex\", \n",
    "                           \"접수내용_regex\", \"사업내용_regex\", \"문의개요_noun\", \"접수내용_noun\", \"사업내용_noun\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file3.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in file3.columns :\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file3.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorization(vectors) :\n",
    "    cnt = len(vectors)\n",
    "    \n",
    "    if cnt == 0:\n",
    "        return np.array([0]*200, dtype=np.float64)\n",
    "    \n",
    "    result = np.array([0]*200, dtype=np.float64)\n",
    "    \n",
    "    for vector in vectors :\n",
    "        result += np.array(vector[1])\n",
    "    \n",
    "    return result/cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorization_with_importanve(vector, importance) :\n",
    "    \n",
    "    result= []\n",
    "    \n",
    "    for v, i in zip(vector, importance) :\n",
    "        temp_result = np.array([0]*200, dtype=np.float64)\n",
    "        temp_total_i = 0\n",
    "        temp_i = dict(i[0])\n",
    "        temp_v = list(v[0])\n",
    "        \n",
    "        for vv in temp_v :\n",
    "            word = vv[0]\n",
    "            \n",
    "            if word in temp_i :  \n",
    "                vec = np.array(vv[1]) * temp_i[word]\n",
    "                temp_result+= vec\n",
    "                temp_total_i +=temp_i[word]\n",
    "                \n",
    "        if temp_total_i == 0:\n",
    "            result.append([temp_result])\n",
    "        else :\n",
    "            vec = temp_result/temp_total_i\n",
    "            result.append([vec])\n",
    "        \n",
    "    return result        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file3[\"문의개요_vector\"]= file3[\"문의개요_vector\"].map(lambda x : vectorization(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file3[\"사업내용_vector\"]= file3[\"사업내용_vector\"].map(lambda x : vectorization(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = pd.DataFrame(vectorization_with_importanve(file3[[\"접수내용_vector\"]].values, \n",
    "                                                    file3[[\"접수내용_importance\"]].values)).rename(columns = {0 : \"vec\"})\n",
    "\n",
    "print(len(result))\n",
    "result.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file3[\"접수내용_vector\"] = result[\"vec\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "file4 = file3.copy().drop(\"접수내용_importance\", axis=1)\n",
    "file4.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file4[\"total_vector\"] = (file4[\"접수내용_vector\"]*2 + file4[\"문의개요_vector\"] + file4[\"사업내용_vector\"]) /4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file5 = file4.copy().drop([\"접수내용_vector\", \"문의개요_vector\", \"사업내용_vector\"], axis=1)\n",
    "file5_success = file5[[\"cancelled\",\"drop\",\"failed\",\"holding\",\"ing\",\"pending\",\"success\"]]\n",
    "file5 = file5.drop([\"cancelled\",\"drop\",\"failed\",\"holding\",\"ing\",\"pending\",\"success\"], axis=1)\n",
    "\n",
    "file5[\"total_vector\"] = file5[\"total_vector\"].map(lambda x: list(x))\n",
    "\n",
    "file5_lst = list(file5.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file5.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = []\n",
    "\n",
    "for elts in file5_lst :\n",
    "    name = elts[0]\n",
    "    vec = list(elts[1:-1]) + list(elts[-1])\n",
    "    \n",
    "    print(len(vec))\n",
    "    result.append([name, vec])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file6 = pd.DataFrame(result).rename(columns = {0:\"company\", 1:\"vector\"})\n",
    "file6[\"vector\"] = file6[\"vector\"].map(lambda x : list(x))\n",
    "\n",
    "file6.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file7 = pd.concat([file6, file5_success], axis=1)\n",
    "file7.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br><br></br><br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto Encoder-100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "AE_input = np.array(list(file7.vector.values))\n",
    "AE_input = AE_input.reshape([5883, 738])\n",
    "\n",
    "print(len(AE_input))\n",
    "print(AE_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "training_epoch = 8000\n",
    "batch_size = 100\n",
    "n_input = 738\n",
    "n_hidden = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_X = tf.placeholder(tf.float32, [None, n_input])\n",
    "encoder = tf.layers.dense(input_X, n_hidden, activation=tf.nn.relu)\n",
    "decoder = tf.layers.dense(encoder, n_input)\n",
    "\n",
    "cost = tf.reduce_mean(tf.pow(input_X-decoder, 2))\n",
    "optimizer = tf.train.AdadeltaOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Learning Start!\\n\")\n",
    "\n",
    "total_batch = int(len(file7)/batch_size)\n",
    "\n",
    "for epoch in range(training_epoch) :\n",
    "    total_cost = 0\n",
    "    idx = 0\n",
    "    \n",
    "    for i in range(total_batch) :\n",
    "        batch_xs =AE_input[idx: idx+batch_size]\n",
    "        idx += batch_size\n",
    "        \n",
    "        _, c = sess.run([optimizer, cost], feed_dict = {input_X : batch_xs})\n",
    "        total_cost += c\n",
    "    \n",
    "    if epoch%100 == 0 :\n",
    "        print(epoch, \"avg :\", total_cost/total_batch)\n",
    "    if epoch==1000 :\n",
    "        returned_sample1 = sess.run([decoder], feed_dict = {input_X : AE_input[0:1]})\n",
    "        \n",
    "    if epoch==2000 :\n",
    "        returned_sample2 = sess.run([decoder], feed_dict = {input_X : AE_input[0:1]})\n",
    "    \n",
    "print(\"\\nLearning End!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = AE_input[0].reshape([18, 41])\n",
    "plt.imshow(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(np.array(returned_sample1).reshape([18, 41]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.array(returned_sample2).reshape([18, 41]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returned_sample3 = sess.run([decoder], feed_dict = {input_X : AE_input[0:1]})\n",
    "plt.imshow(np.array(returned_sample3).reshape([18, 41]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "encoded_vector = []\n",
    "\n",
    "for data in AE_input :\n",
    "    rlt = sess.run([encoder], feed_dict = {input_X : data.reshape([1, 738])})\n",
    "    encoded_vector.append(rlt)\n",
    "    \n",
    "print(len(encoded_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encding_vector = pd.DataFrame(encoded_vector).rename(columns = {0 : \"vector\"})\n",
    "encding_vector[\"vector\"] = encding_vector[\"vector\"].map(lambda x : x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file7[\"encoded_vector_100\"] = encding_vector[\"vector\"]\n",
    "file7.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "saver.save(sess, './AE_model_100/AE100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file7.to_csv(\"vectored_data_AE100.csv\")\n",
    "file7.to_excel(\"vectored_data_AE100.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br><br></br><br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto Encoder-50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AE_input = np.array(list(file7.vector.values))\n",
    "AE_input = AE_input.reshape([5883, 738])\n",
    "\n",
    "print(len(AE_input))\n",
    "print(AE_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "training_epoch = 8000\n",
    "batch_size = 100\n",
    "n_input = 738\n",
    "n_hidden = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_X = tf.placeholder(tf.float32, [None, n_input])\n",
    "encoder = tf.layers.dense(input_X, n_hidden, activation=tf.nn.relu)\n",
    "decoder = tf.layers.dense(encoder, n_input)\n",
    "\n",
    "cost = tf.reduce_mean(tf.pow(input_X-decoder, 2))\n",
    "optimizer = tf.train.AdadeltaOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Learning Start!\\n\")\n",
    "\n",
    "total_batch = int(len(file7)/batch_size)\n",
    "\n",
    "for epoch in range(training_epoch) :\n",
    "    total_cost = 0\n",
    "    idx = 0\n",
    "    \n",
    "    for i in range(total_batch) :\n",
    "        batch_xs =AE_input[idx: idx+batch_size]\n",
    "        idx += batch_size\n",
    "        \n",
    "        _, c = sess.run([optimizer, cost], feed_dict = {input_X : batch_xs})\n",
    "        total_cost += c\n",
    "    \n",
    "    if epoch%100 == 0 :\n",
    "        print(epoch, \"avg :\", total_cost/total_batch)\n",
    "    if epoch==1000 :\n",
    "        returned_sample1 = sess.run([decoder], feed_dict = {input_X : AE_input[0:1]})\n",
    "        \n",
    "    if epoch==2000 :\n",
    "        returned_sample2 = sess.run([decoder], feed_dict = {input_X : AE_input[0:1]})\n",
    "    \n",
    "print(\"\\nLearning End!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = AE_input[0].reshape([18, 41])\n",
    "plt.imshow(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.array(returned_sample1).reshape([18, 41]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.array(returned_sample2).reshape([18, 41]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returned_sample3 = sess.run([decoder], feed_dict = {input_X : AE_input[0:1]})\n",
    "plt.imshow(np.array(returned_sample3).reshape([18, 41]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_vector2 = []\n",
    "\n",
    "for data in AE_input :\n",
    "    rlt = sess.run([encoder], feed_dict = {input_X : data.reshape([1, 738])})\n",
    "    encoded_vector2.append(rlt)\n",
    "    \n",
    "print(len(encoded_vector2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encding_vector2 = pd.DataFrame(encoded_vector2).rename(columns = {0 : \"vector\"})\n",
    "encding_vector2[\"vector\"] = encding_vector2[\"vector\"].map(lambda x : x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file7[\"encoded_vector_50\"] = encding_vector2[\"vector\"]\n",
    "file7.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "saver.save(sess, './AE_model_50/AE50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file7.to_csv(\"vectored_data_AE100_AE50.csv\")\n",
    "file7.to_excel(\"vectored_data_AE100_AE50.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br><br></br><br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA\n",
    "- http://antilibrary.org/1091"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 성능이 autoencoder보다 좋지 않기 때문에 사용하지 않음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br><br></br><br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## making dataftame form vector for ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_list(vector):\n",
    "    temp = vector.replace(\" \", \",\").replace(\"\\n\", \",\")\n",
    "    \n",
    "    while temp.count(\",,\") :\n",
    "        temp = temp.replace(\",,\", \",\")\n",
    "    \n",
    "    temp = temp.replace(\"[,\", \"[\").replace(\",]\", \"]\")\n",
    "    \n",
    "    return eval(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "file7 = pd.read_csv(\"vectored_data_AE100_AE50.csv\").drop(\"Unnamed: 0\",axis=1)\n",
    "file7.head(10)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "file7 = pd.read_csv(\"vectored_data_AE100_AE50.csv\").drop(\"Unnamed: 0\",axis=1)\n",
    "file7[\"vector\"] = file7[\"vector\"].map(str_to_list)\n",
    "file7[\"encoded_vector_100\"] = file7[\"encoded_vector_100\"].map(str_to_list)\n",
    "file7[\"encoded_vector_50\"] = file7[\"encoded_vector_50\"].map(str_to_list)\n",
    "file7.head(10)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file8 = file7.copy()\n",
    "file8.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_ml_50 = file8[[\"company\", 'encoded_vector_50', \"success\"]]\n",
    "df_for_ml_100 = file8[[\"company\", 'encoded_vector_100', \"success\"]]\n",
    "df_for_ml_738 =  file8[[\"company\", 'vector', \"success\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_ml_50 = pd.concat([df_for_ml_50,\n",
    "                          pd.DataFrame(df_for_ml_50['encoded_vector_50'].values.tolist(), columns=[str(idx) for idx in range(1,51)])],\n",
    "                         axis=1).drop(\"encoded_vector_50\", axis=1)\n",
    "\n",
    "df_for_ml_100 = pd.concat([df_for_ml_100,\n",
    "                          pd.DataFrame(df_for_ml_100['encoded_vector_100'].values.tolist(), columns=[str(idx) for idx in range(1,101)])],\n",
    "                         axis=1).drop(\"encoded_vector_100\", axis=1)\n",
    "\n",
    "df_for_ml_738 = pd.concat([df_for_ml_738,\n",
    "                          pd.DataFrame(df_for_ml_738['vector'].values.tolist(), columns=[str(idx) for idx in range(1,739)])],\n",
    "                         axis=1).drop(\"vector\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_ml_50.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_for_ml_100.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_ml_738.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_50 = [str(idx) for idx in range(1,51)]\n",
    "features_100 = [str(idx) for idx in range(1,101)]\n",
    "features_738 = [str(idx) for idx in range(1,739)]\n",
    "target = \"success\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_50 = df_for_ml_50[:int(len(df_for_ml_50)/5*4)]\n",
    "test_50 = df_for_ml_50[int(len(df_for_ml_50)/5*4):]\n",
    "\n",
    "training_100 = df_for_ml_100[:int(len(df_for_ml_100)/5*4)]\n",
    "test_100 = df_for_ml_100[int(len(df_for_ml_100)/5*4):]\n",
    "\n",
    "training_738 = df_for_ml_738[:int(len(df_for_ml_738)/5*4)]\n",
    "test_738 = df_for_ml_738[int(len(df_for_ml_738)/5*4):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br><br></br><br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## xgboost, catboost, lgboost, random forest, logistic regression\n",
    "- using not encoded vector\n",
    "- to find dominant variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model_738 = XGBClassifier(max_depth=20, n_estimator=1500)\n",
    "lgb_model_738 = LGBMClassifier(max_depth=20, n_estimator=1500, min_data=1)\n",
    "cat_model_738 = CatBoostClassifier(max_depth=16, iterations=1500)\n",
    "rf_model_738 = RandomForestClassifier(max_depth=20, n_estimators=1500)\n",
    "lr_model_738 = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xgb_model_738.fit(training_738[features_738], training_738[target])\n",
    "lgb_model_738.fit(training_738[features_738], training_738[target])\n",
    "cat_model_738.fit(training_738[features_738], training_738[target])\n",
    "rf_model_738.fit(training_738[features_738], training_738[target])\n",
    "lr_model_738.fit(training_738[features_738], training_738[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(xgb_model_738.score(test_738[features_738], test_738[target]))\n",
    "print(lgb_model_738.score(test_738[features_738], test_738[target]))\n",
    "print(cat_model_738.score(test_738[features_738], test_738[target]))\n",
    "print(rf_model_738.score(training_738[features_738], training_738[target]))\n",
    "print(lr_model_738.score(training_738[features_738], training_738[target]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(xgb_model_738, \"./model/model_738/xgb_model_738.pkl\")\n",
    "joblib.dump(lgb_model_738, \"./model/model_738/lgb_model_738.pkl\")\n",
    "joblib.dump(cat_model_738, \"./model/model_738/cat_model_738.pkl\")\n",
    "joblib.dump(rf_model_738, \"./model/model_738/rf_model_738.pkl\")\n",
    "joblib.dump(lr_model_738, \"./model/model_738/lr_model_738.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(xgb_model_738.feature_importances_).iplot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(lgb_model_738.feature_importances_).iplot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(cat_model_738.feature_importances_).iplot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(rf_model_738.feature_importances_).iplot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br><br></br><br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## xgboost, catboost, lgboost, random forest, logistic regression\n",
    "- using encoded vector\n",
    "- to find dominant variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model_100 = XGBClassifier(max_depth=12, n_estimator=1500)\n",
    "lgb_model_100 = LGBMClassifier(max_depth=12, n_estimator=1500, min_data=1)\n",
    "cat_model_100 = CatBoostClassifier(max_depth=12, iterations=1500)\n",
    "rf_model_100 = RandomForestClassifier(max_depth=12, n_estimators=1500)\n",
    "lr_model_100 = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xgb_model_100.fit(training_100[features_100], training_100[target])\n",
    "lgb_model_100.fit(training_100[features_100], training_100[target])\n",
    "cat_model_100.fit(training_100[features_100], training_100[target])\n",
    "rf_model_100.fit(training_100[features_100], training_100[target])\n",
    "lr_model_100.fit(training_100[features_100], training_100[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xgb_model_100.score(test_100[features_100], test_100[target]))\n",
    "print(lgb_model_100.score(test_100[features_100], test_100[target]))\n",
    "print(cat_model_100.score(test_100[features_100], test_100[target]))\n",
    "print(rf_model_100.score(test_100[features_100], test_100[target]))\n",
    "print(lr_model_100.score(test_100[features_100], test_100[target]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(xgb_model_100, \"./model/model_100/xgb_model_100.pkl\")\n",
    "joblib.dump(lgb_model_100, \"./model/model_100/lgb_model_100.pkl\")\n",
    "joblib.dump(cat_model_100, \"./model/model_100/cat_model_100.pkl\")\n",
    "joblib.dump(rf_model_100, \"./model/model_100/rf_model_100.pkl\")\n",
    "joblib.dump(lr_model_100, \"./model/model_100/lr_model_100.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "xgb_model_50 = XGBClassifier(max_depth=6, n_estimator=1500)\n",
    "lgb_model_50 = LGBMClassifier(max_depth=6, n_estimator=1500, min_data=1)\n",
    "cat_model_50 = CatBoostClassifier(max_depth=6, iterations=1500)\n",
    "rf_model_50 = RandomForestClassifier(max_depth=6, n_estimators=1500)\n",
    "lr_model_50 = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xgb_model_50.fit(training_50[features_50], training_50[target])\n",
    "lgb_model_50.fit(training_50[features_50], training_50[target])\n",
    "cat_model_50.fit(training_50[features_50], training_50[target])\n",
    "rf_model_50.fit(training_50[features_50], training_50[target])\n",
    "lr_model_50.fit(training_50[features_50], training_50[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xgb_model_50.score(test_50[features_50], test_50[target]))\n",
    "print(lgb_model_50.score(test_50[features_50], test_50[target]))\n",
    "print(cat_model_50.score(test_50[features_50], test_50[target]))\n",
    "print(rf_model_50.score(training_50[features_50], training_50[target]))\n",
    "print(lr_model_50.score(training_50[features_50], training_50[target]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(xgb_model_50, \"./model/model_50/xgb_model_50.pkl\")\n",
    "joblib.dump(lgb_model_50, \"./model/model_50/lgb_model_50.pkl\")\n",
    "joblib.dump(cat_model_50, \"./model/model_50/cat_model_50.pkl\")\n",
    "joblib.dump(rf_model_50, \"./model/model_50/rf_model_50.pkl\")\n",
    "joblib.dump(lr_model_50, \"./model/model_50/lr_model_50.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br><br></br><br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "- using endoded vector\n",
    "- 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdbscan_50 = DBSCAN()\n",
    "hdbscan_100 = DBSCAN()\n",
    "hdbscan_738 = DBSCAN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdbscan_50_result = hdbscan_50.fit_predict(df_for_ml_50[features_50])\n",
    "hdbscan_100_result = hdbscan_100.fit_predict(df_for_ml_100[features_100])\n",
    "hdbscan_738_result = hdbscan_738.fit_predict(df_for_ml_738[features_738])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdbscan_50_df = pd.DataFrame(hdbscan_50_result).rename(columns = {0 : \"hdbscan_cluster_50\"})\n",
    "hdbscan_100_df = pd.DataFrame(hdbscan_100_result).rename(columns = {0 : \"hdbscan_cluster_100\"})\n",
    "hdbscan_738_df = pd.DataFrame(hdbscan_738_result).rename(columns = {0 : \"hdbscan_cluster_738\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdbscan_50_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdbscan_100_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdbscan_738_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_50 = KMeans(7)\n",
    "kmeans_100 = KMeans(7)\n",
    "kmeans_738 = KMeans(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_50_result = kmeans_50.fit_predict(df_for_ml_50[features_50])\n",
    "kmeans_100_result = kmeans_100.fit_predict(df_for_ml_100[features_100])\n",
    "kmeans_738_result = kmeans_738.fit_predict(df_for_ml_738[features_738])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_50_df = pd.DataFrame(kmeans_50_result).rename(columns = {0 : \"kmeans_cluster_50\"})\n",
    "kmeans_100_df = pd.DataFrame(kmeans_100_result).rename(columns = {0 : \"kmeans_cluster_100\"})\n",
    "kmeans_738_df = pd.DataFrame(kmeans_738_result).rename(columns = {0 : \"kmeans_cluster_738\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_50_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_100_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_100_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file9 = file8.copy()\n",
    "file9 = pd.concat([file9, hdbscan_50_df, hdbscan_100_df, hdbscan_738_df, kmeans_50_df, kmeans_100_df, kmeans_738_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file9.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file9.to_excel(\"vectored_data_AE100_AE50_cluster.xlsx\")\n",
    "file9.to_csv(\"vectored_data_AE100_AE50_cluster.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br><br></br><br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP\n",
    "- using endoded vector(50 dims)\n",
    "    - 다른 vector들은 0값이 너무 많아서 사용불가\n",
    "- to predict 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(lst, num_class=2) :\n",
    "    return np.eye(num_class)[lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file10 = shuffle(file9[[\"encoded_vector_50\", \"success\"]])\n",
    "file10.to_csv(\"vectored_data_AE50_for_MLP.csv\")\n",
    "\n",
    "print(len(file10[file10[\"success\"]== 1]))\n",
    "file10.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_input = np.array(list(file10[\"encoded_vector_50\"].values))\n",
    "print(total_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_label = one_hot(np.array(list(file10[\"success\"].values)))\n",
    "print(total_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_input = total_input[:int(len(total_data)/5)*4]\n",
    "test_input = total_input[int(len(total_data)/5)*4:]\n",
    "\n",
    "training_label = total_label[:int(len(total_data)/5)*4]\n",
    "test_label = total_label[int(len(total_data)/5)*4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.05\n",
    "training_epoch = 100\n",
    "batch_size = 100\n",
    "n_input = 50\n",
    "n_output = 2\n",
    "n_hidden = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_X = tf.placeholder(tf.float32, [None, n_input])\n",
    "input_Y = tf.placeholder(tf.float32, [None, n_output])\n",
    "training = tf.placeholder(tf.bool)\n",
    "\n",
    "layer1 = tf.layers.dense(input_X, n_hidden, activation=tf.nn.relu)\n",
    "dropout1 = tf.layers.dropout(layer1, training=training)\n",
    "\n",
    "layer2 = tf.layers.dense(dropout1, n_hidden, activation=tf.nn.relu)\n",
    "dropout2 = tf.layers.dropout(layer2, training=training)\n",
    "\n",
    "layer3 = tf.layers.dense(dropout2, n_output, activation=tf.nn.relu)\n",
    "\n",
    "cost = tf.reduce_mean(tf.pow(input_Y-layer3, 2))\n",
    "optimizer = tf.train.AdadeltaOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(layer3, 1), tf.argmax(input_Y, 1))     \n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "print(\"Learning Start!\\n\")\n",
    "\n",
    "total_batch = int(len(training_input)/batch_size)\n",
    "\n",
    "for epoch in range(training_epoch) :\n",
    "    total_cost = 0\n",
    "    idx = 0\n",
    "    \n",
    "    for i in range(total_batch) :\n",
    "        batch_xs, batch_ys = training_input[idx: idx+batch_size], training_label[idx: idx+batch_size]\n",
    "        idx += batch_size\n",
    "        \n",
    "        _, c = sess.run([optimizer, cost], feed_dict = {input_X : batch_xs, input_Y : batch_ys, training : True})\n",
    "        total_cost += c\n",
    "    \n",
    "    if epoch%10 == 0 :\n",
    "        print(epoch, \"avg :\", total_cost/total_batch)\n",
    "        print(\"accuracy :\" , sess.run([accuracy], feed_dict = {input_X : test_input, input_Y : test_label, training : False}))\n",
    "    \n",
    "print(\"\\nLearning End!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "saver.save(sess, './MLP_model_50/MLP_50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br><br></br><br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN \n",
    "- using encoded vector\n",
    "- to predict 결과\n",
    "- CNN for 정형 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br><br></br><br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 아이디어\n",
    "- 연도별 기업, 키워드의 워드 클라우드를 통한 트렌드 분석\n",
    "- 각 column의 연도별 변화 시각화\n",
    "- 지도 시각화\n",
    "- 새로운 지표 개발\n",
    "- 자료 정리방식 추천\n",
    "- 각 column 사이의 correlation\n",
    "- 그외 버린 column들의 정보 요약\n",
    "- 조건부 확률을 활용하여 어느 경우에 가장 success확률이 높은지\n",
    "- 상대 기업의 크기에 대한 정보(어떤 기업이랑 해야 drop이 낮은지, 연도별로 상대기업의 크기가 어떻게 됐는지)\n",
    "- 어느 곳에 입찰을 들어가거나 계약을 맺어야 성공률이 높은지 분석해서 인적자원 관리시스템 구축 전략 제시(확률적 접근)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
