{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 네이버 뉴스 크롤러"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import threading\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaverNewsCrawler(threading.Thread) :\n",
    "    def __init__(self):\n",
    "        threading.Thread.__init__(self) \n",
    "    \n",
    "    def setParams(self, keyword, start, end, sort=1, include = [], not_include = []) :\n",
    "        # 검색 키워드\n",
    "        # 시작하는 뉴스 번호(포함)\n",
    "        # 끝나는 뉴스 번호(비포함)\n",
    "        # 정렬방법 : 0=관련도순, 1=최신순, 2=오래된순\n",
    "        # 반드시 포함해야하는 단어\n",
    "        # 반드시 제외해야하는 단어\n",
    "        self.keyword = keyword\n",
    "        self.start_num = start\n",
    "        self.end_num = end\n",
    "        self.sort =sort\n",
    "        self.include = include\n",
    "        self.not_include = not_include\n",
    "        \n",
    "        # header가 없으면 차단당함\n",
    "        self.headers =  {\"Referer\": \"https://m.search.naver.com/search.naver?where=m_news\".encode('utf-8'),\n",
    "                         \"User-Agent\": \"Mozilla/5.0 (Linux; Android 7.0; SM-G930V Build/NRD90M) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/59.0.3071.125 Mobile Safari/537.36\"}\n",
    "        \n",
    "        self.title = []\n",
    "        self.link = []\n",
    "        self.date = []\n",
    "        self.publisher = []\n",
    "        self.contents = []\n",
    "        \n",
    "    def run(self):\n",
    "        self.start_metadata_crawling()\n",
    "        self.start_contents_crawling()\n",
    "        self.filtering()\n",
    "        \n",
    "    def start_metadata_crawling(self) :\n",
    "        \"\"\"뉴스의 링크 크롤링\"\"\"\n",
    "        \n",
    "        # 뉴스 페이지 하나당 15개의 뉴스가 존재하므로\n",
    "        # 한 페이지당10개씩 크롤링\n",
    "        for idx in range(self.start_num, self.end_num, 15) :\n",
    "            self.news = \"https://m.search.naver.com/search.naver?where=m_news&query={}&start={}&sort={}\".format(self.keyword, self.start_num, self.sort)\n",
    "\n",
    "            req = requests.get(self.news, headers=self.headers)\n",
    "            html = req.text\n",
    "            soup = bs(html, 'html.parser')\n",
    "            \n",
    "            title, link, date, publisher  = self.parsing_metadata(soup) \n",
    "            \n",
    "            self.title += title\n",
    "            self.link += link\n",
    "            self.date += date\n",
    "            self.publisher += publisher\n",
    "            \n",
    "    def start_contents_crawling(self) : \n",
    "        \"\"\"뉴스의 내용 크롤링\"\"\"\n",
    "        \n",
    "        for link in self.link :\n",
    "            req = requests.get(link, headers=self.headers)\n",
    "            new_link = req.url\n",
    "\n",
    "            req2= requests.get(new_link, headers=self.headers)\n",
    "            html = req2.text\n",
    "            soup = bs(html, 'html.parser')\n",
    "            \n",
    "            # 연예 기사\n",
    "            if \"m.entertain.naver\" in new_link :\n",
    "                try :    \n",
    "                    contents = self.parsing_entertain(soup)\n",
    "                except :\n",
    "                    contents = \"\"\n",
    "                \n",
    "            # 스포츠 기사\n",
    "            elif \"m.sports.naver\" in new_link: \n",
    "                try :\n",
    "                    contents = self.parsing_sports(soup)\n",
    "                except :\n",
    "                    contents = req.url\n",
    "            \n",
    "            # 일반 뉴스 기사\n",
    "            elif \"m.news.naver\" in new_link :\n",
    "                try :\n",
    "                    contents = self.parsing_news(soup)\n",
    "                except :\n",
    "                    contents = req.url\n",
    "                \n",
    "            else :\n",
    "                contents = req.url\n",
    "                \n",
    "            self.contents.append(contents)\n",
    "            \n",
    "    def filtering(self) :\n",
    "        pass\n",
    "            \n",
    "    def parsing_entertain(self, soup) :\n",
    "        contents = soup.find('div', {'class': 'newsct_article go_trans'}).text\n",
    "        return contents\n",
    "    \n",
    "    def parsing_sports(self, soup) :\n",
    "        contents = soup.find('article', {'class': 'main_article'}).text\n",
    "        return contents\n",
    "        \n",
    "    def parsing_news(self, soup) :\n",
    "        contents = soup.find('div', {'id': 'dic_area'}).text\n",
    "        return contents\n",
    "        \n",
    "    def parsing_metadata(self, soup) :\n",
    "        title = []\n",
    "        link = []\n",
    "        date = []\n",
    "        publisher = []\n",
    "        \n",
    "        wraps = soup.find_all('div', {'class': \"news_wrap\"})\n",
    "        \n",
    "        for wrap in wraps :\n",
    "            \n",
    "            # 네이버 뉴스인지 아닌지 확인\n",
    "            # 길이가 2이면 네이버뉴스\n",
    "            p = wrap.findAll(\"cite\", {\"class\" : \"sub_txt\"})\n",
    "            if len(p) == 2 :\n",
    "                title.append(wrap.find(\"div\", {\"class\" : \"api_txt_lines tit\"}).text)\n",
    "                link.append(wrap.find(\"a\", {\"class\" : \"news_tit\"}).get(\"href\"))\n",
    "                date.append(wrap.find(\"span\", {\"class\" : \"sub_txt sub_time\"}).text)\n",
    "                publisher.append(p[0].text)\n",
    "            \n",
    "        return title, link, date, publisher\n",
    "    \n",
    "    def getData(self) :\n",
    "        return self.title ,self.link, self.date, self.publisher, self.contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news_num(keyword) : \n",
    "    news = \"https://search.naver.com/search.naver?where=news&query={}\".format(keyword)\n",
    "    \n",
    "    try :\n",
    "        req = requests.get(news)\n",
    "        html = req.text\n",
    "        soup = bs(html, 'html.parser')\n",
    "\n",
    "        text = soup.find(\"div\", {\"class\" : \"title_desc all_my\"}).text\n",
    "        num = int(text.split(\"/\")[1].replace(\",\" , \"\").replace(\"건\", \"\").strip())\n",
    "        \n",
    "    except :\n",
    "        num = 0\n",
    "        \n",
    "    return num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawling_Naver(keyword, num_thread, num=None, sort=1, include = [], not_include = []) :\n",
    "    if not num :\n",
    "        num = get_news_num(keyword)\n",
    "        \n",
    "    if num_thread > 1 :\n",
    "        num_thread -= 1\n",
    "        \n",
    "    interval = num//num_thread\n",
    "    thread_lst = []\n",
    "    num_lst = []\n",
    "    \n",
    "    for start in range(1, num, interval) :\n",
    "        naver = NaverNewsCrawler()\n",
    "        naver.setParams(keyword, start, start+interval)\n",
    "        naver.daemon = True\n",
    "        naver.start()\n",
    "        \n",
    "        thread_lst.append(naver)\n",
    "        num_lst.append((start, start+interval))\n",
    "        \n",
    "    print(\"Num :\", num)\n",
    "    print(\"Num_Thread :\", num_thread)\n",
    "    print(\"Num_lst :\", num_lst)\n",
    "    print(\"\")\n",
    "    \n",
    "    ## wait for thread\n",
    "    idx =1\n",
    "    for thread in thread_lst :\n",
    "        thread.join()\n",
    "        print(idx, \"Thread finished\")\n",
    "        idx += 1\n",
    "        \n",
    "    tt = []\n",
    "    ll = []\n",
    "    dd = []\n",
    "    pp = []\n",
    "    cc = []\n",
    "    for thread in thread_lst : \n",
    "        t,l,d,p,c = thread.getData()\n",
    "        tt += t\n",
    "        ll += l\n",
    "        dd += d\n",
    "        pp += p\n",
    "        cc += c\n",
    "        \n",
    "\n",
    "    return tt, ll, dd, pp, cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_result(result, path=\"./\", filename=\"crawling.csv\") :\n",
    "    \n",
    "    data = []\n",
    "    for t,l,d,p,c in zip(result[0],result[1], result[2], result[3], result[4]) :\n",
    "        data.append([t,l,d,p,c])\n",
    "        \n",
    "    df = pd.DataFrame(data, columns=[\"title\", \"link\", \"date\", \"publisher\", \"contents\"])\n",
    "    df.to_csv(path + filename)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num : 15\n",
      "Num_Thread : 1\n",
      "Num_lst : [(1, 16)]\n",
      "\n",
      "1 Thread finished\n"
     ]
    }
   ],
   "source": [
    "result = crawling_Naver(\"크롤링\", 1, num=15)\n",
    "df = save_result(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
