{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity = pd.read_csv(\"train_activity.csv\").drop(\"Unnamed: 0\", axis=1)\n",
    "label = pd.read_csv(\"train_label.csv\").drop(\"Unnamed: 0\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "label.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br><br></br><br></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_lst = sorted(list(label[\"acc_id\"]))\n",
    "\n",
    "print(len(id_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_lst = [0]+ list(activity.min())[3:]\n",
    "\n",
    "print(len(activity.columns))\n",
    "print(len(min_lst))\n",
    "print(min_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_dic = {}\n",
    "\n",
    "for i in id_lst :\n",
    "    for idx in range(1,9) : \n",
    "        activity_dic[(idx, (int(i)))] = min_lst\n",
    "        \n",
    "print(len(activity_dic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in activity.values :\n",
    "    activity_dic[(int(data[0]), int(data[1]))] = data[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_lst = [list(y) for y in activity_dic.values()]\n",
    "activity_lst[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(activity.columns[2:])\n",
    "col_dic = {}\n",
    "\n",
    "for idx, col in enumerate(cols) :\n",
    "    col_dic[idx] = col "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity.sort_values(by=[\"acc_id\", \"wk\"]).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "activity2 = pd.DataFrame(activity_lst).rename(columns=col_dic)\n",
    "\n",
    "print(len(activity2))\n",
    "activity2.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dic = {\"week\":0 , \"month\" :1, \"2month\":2, \"retained\":3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2 = label.sort_values(by=\"acc_id\")\n",
    "label2[\"label\"] = label2[\"label\"].map(lambda x : label_dic[x])\n",
    "\n",
    "label2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br><br></br><br></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(lst, num_class=4) :\n",
    "    return np.eye(num_class)[lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_lst = []\n",
    "wk = 8\n",
    "\n",
    "for now in range(0,800001,8) :\n",
    "    if now == 0 :\n",
    "        last = now\n",
    "        continue\n",
    "    \n",
    "    total_lst.append(activity_lst[last:now])\n",
    "    last=now\n",
    "    \n",
    "print(len(total_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_label = one_hot(label2.label.values)\n",
    "print(len(total_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_lst = np.array(total_lst[:80000])\n",
    "valid_lst = np.array(total_lst[80000:90000])\n",
    "test_lst = np.array(total_lst[90000:])\n",
    "\n",
    "training_label = np.array(total_label[:80000])\n",
    "valid_label = np.array(total_label[80000:90000])\n",
    "test_label = np.array(total_label[90000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(training_lst[0]))\n",
    "training_lst[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br><br></br><br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_length = 8\n",
    "input_size = 36\n",
    "input_class = 4\n",
    "hidden_layer1 = 64\n",
    "hidden_layer2 = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model1() :\n",
    "    def __init__(self, sess, name):\n",
    "        self.sess = sess\n",
    "        self.name = name\n",
    "  \n",
    "    def build(self) :\n",
    "        with tf.variable_scope(self.name) :\n",
    "            \n",
    "            self.X = tf.placeholder(tf.float32, [None, input_length, input_size])\n",
    "            self.Y = tf.placeholder(tf.float32, [None, input_class])\n",
    "            self.learning_rate =  tf.placeholder(tf.float32)\n",
    "            self.training = tf.placeholder(tf.bool)\n",
    "            \n",
    "            cell1 = tf.nn.rnn_cell.BasicLSTMCell(hidden_layer1)\n",
    "            dropout1 = tf.nn.rnn_cell.DropoutWrapper(cell1, output_keep_prob=0.5)\n",
    "            cell2 = tf.nn.rnn_cell.BasicLSTMCell(hidden_layer1)\n",
    "            multi_cell = tf.nn.rnn_cell.MultiRNNCell([dropout1, cell2])\n",
    "            \n",
    "            output, state = tf.nn.dynamic_rnn(multi_cell, self.X, dtype=tf.float32)\n",
    "            output = tf.transpose(output,[1,0,2])[-1]\n",
    "            \n",
    "            dense = tf.layers.dense(inputs=output, units=input_class)\n",
    "            self.logits = dense\n",
    "\n",
    "            self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=self.logits, labels=self.Y))\n",
    "            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope=self.name)\n",
    "            \n",
    "            with tf.control_dependencies(update_ops):\n",
    "                self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.cost)\n",
    "\n",
    "            correct_prediction = tf.equal(tf.argmax(self.logits, 1), tf.argmax(self.Y, 1))     \n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    def predict(self, X_input, training=False):\n",
    "        return self.sess.run(self.logits,feed_dict={self.X: X_input, self.training: training})\n",
    "\n",
    "    def get_accuracy(self, X_input, Y_input, training=False):\n",
    "        return self.sess.run(self.accuracy,feed_dict={self.X: X_input,self.Y: Y_input, self.training: training})\n",
    "\n",
    "    def train(self, X_input, Y_input, learning_rate,training=True):\n",
    "        return self.sess.run([self.cost, self.optimizer], feed_dict={self.X: X_input, self.Y: Y_input, self.learning_rate:learning_rate,self.training: training})\n",
    "    \n",
    "    def evaluate(self, X_input, Y_input, batch_size):\n",
    "        N = X_input.shape[0]\n",
    "            \n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "            \n",
    "        for i in range(0, N, batch_size):\n",
    "            X_batch = X_input[i:i + batch_size]\n",
    "            Y_batch = Y_input[i:i + batch_size]\n",
    "                \n",
    "            feed_dict = {self.X: X_batch, self.Y: Y_batch, self.training: False}\n",
    "                \n",
    "            loss = self.cost\n",
    "            accuracy = self.accuracy\n",
    "                \n",
    "            step_loss, step_acc = self.sess.run([loss, accuracy], feed_dict=feed_dict)\n",
    "                \n",
    "            total_loss += step_loss * X_batch.shape[0]\n",
    "            total_acc += step_acc * X_batch.shape[0]\n",
    "            \n",
    "        total_loss /= N\n",
    "        total_acc /= N\n",
    "            \n",
    "        return total_loss, total_acc\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model2() :\n",
    "    def __init__(self, sess, name):\n",
    "        self.sess = sess\n",
    "        self.name = name\n",
    "  \n",
    "    def build(self) :\n",
    "        with tf.variable_scope(self.name) :\n",
    "            \n",
    "            self.X = tf.placeholder(tf.float32, [None, input_length, input_size])\n",
    "            self.Y = tf.placeholder(tf.float32, [None, input_class])\n",
    "            self.learning_rate =  tf.placeholder(tf.float32)\n",
    "            self.training = tf.placeholder(tf.bool)\n",
    "            \n",
    "            cell1 = tf.nn.rnn_cell.BasicLSTMCell(hidden_layer2)\n",
    "            dropout1 = tf.nn.rnn_cell.DropoutWrapper(cell1, output_keep_prob=0.5)\n",
    "            cell2 = tf.nn.rnn_cell.BasicLSTMCell(hidden_layer2)\n",
    "            multi_cell = tf.nn.rnn_cell.MultiRNNCell([dropout1, cell2])\n",
    "            \n",
    "            output, state = tf.nn.dynamic_rnn(multi_cell, self.X, dtype=tf.float32)\n",
    "            output = tf.transpose(output,[1,0,2])[-1]\n",
    "            \n",
    "            dense = tf.layers.dense(inputs=output, units=input_class)\n",
    "            self.logits = dense\n",
    "\n",
    "            self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=self.logits, labels=self.Y))\n",
    "            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope=self.name)\n",
    "            \n",
    "            with tf.control_dependencies(update_ops):\n",
    "                self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.cost)\n",
    "\n",
    "            correct_prediction = tf.equal(tf.argmax(self.logits, 1), tf.argmax(self.Y, 1))     \n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    def predict(self, X_input, training=False):\n",
    "        return self.sess.run(self.logits,feed_dict={self.X: X_input, self.training: training})\n",
    "\n",
    "    def get_accuracy(self, X_input, Y_input, training=False):\n",
    "        return self.sess.run(self.accuracy,feed_dict={self.X: X_input,self.Y: Y_input, self.training: training})\n",
    "\n",
    "    def train(self, X_input, Y_input, learning_rate,training=True):\n",
    "        return self.sess.run([self.cost, self.optimizer], feed_dict={self.X: X_input, self.Y: Y_input, self.learning_rate:learning_rate,self.training: training})\n",
    "    \n",
    "    def evaluate(self, X_input, Y_input, batch_size):\n",
    "        N = X_input.shape[0]\n",
    "            \n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "            \n",
    "        for i in range(0, N, batch_size):\n",
    "            X_batch = X_input[i:i + batch_size]\n",
    "            Y_batch = Y_input[i:i + batch_size]\n",
    "                \n",
    "            feed_dict = {self.X: X_batch, self.Y: Y_batch, self.training: False}\n",
    "                \n",
    "            loss = self.cost\n",
    "            accuracy = self.accuracy\n",
    "                \n",
    "            step_loss, step_acc = self.sess.run([loss, accuracy], feed_dict=feed_dict)\n",
    "                \n",
    "            total_loss += step_loss * X_batch.shape[0]\n",
    "            total_acc += step_acc * X_batch.shape[0]\n",
    "            \n",
    "        total_loss /= N\n",
    "        total_acc /= N\n",
    "            \n",
    "        return total_loss, total_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model3() :\n",
    "    def __init__(self, sess, name):\n",
    "        self.sess = sess\n",
    "        self.name = name\n",
    "  \n",
    "    def build(self) :\n",
    "        with tf.variable_scope(self.name) :\n",
    "            \n",
    "            self.X = tf.placeholder(tf.float32, [None, input_length, input_size])\n",
    "            self.Y = tf.placeholder(tf.float32, [None, input_class])\n",
    "            self.learning_rate =  tf.placeholder(tf.float32)\n",
    "            self.training = tf.placeholder(tf.bool)\n",
    "            \n",
    "            cell1 = tf.nn.rnn_cell.BasicLSTMCell(hidden_layer1)\n",
    "            dropout1 = tf.nn.rnn_cell.DropoutWrapper(cell1, output_keep_prob=0.5)\n",
    "            cell2 = tf.nn.rnn_cell.BasicLSTMCell(hidden_layer1)\n",
    "            dropout2 = tf.nn.rnn_cell.DropoutWrapper(cell2, output_keep_prob=0.5)\n",
    "            cell3 = tf.nn.rnn_cell.BasicLSTMCell(hidden_layer1)\n",
    "            \n",
    "            multi_cell = tf.nn.rnn_cell.MultiRNNCell([dropout1, dropout2, cell3])\n",
    "            \n",
    "            output, state = tf.nn.dynamic_rnn(multi_cell, self.X, dtype=tf.float32)\n",
    "            output = tf.transpose(output,[1,0,2])[-1]\n",
    "            \n",
    "            dense = tf.layers.dense(inputs=output, units=input_class)\n",
    "            self.logits = dense\n",
    "\n",
    "            self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=self.logits, labels=self.Y))\n",
    "            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope=self.name)\n",
    "            \n",
    "            with tf.control_dependencies(update_ops):\n",
    "                self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.cost)\n",
    "\n",
    "            correct_prediction = tf.equal(tf.argmax(self.logits, 1), tf.argmax(self.Y, 1))     \n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    def predict(self, X_input, training=False):\n",
    "        return self.sess.run(self.logits,feed_dict={self.X: X_input, self.training: training})\n",
    "\n",
    "    def get_accuracy(self, X_input, Y_input, training=False):\n",
    "        return self.sess.run(self.accuracy,feed_dict={self.X: X_input,self.Y: Y_input, self.training: training})\n",
    "\n",
    "    def train(self, X_input, Y_input, learning_rate,training=True):\n",
    "        return self.sess.run([self.cost, self.optimizer], feed_dict={self.X: X_input, self.Y: Y_input, self.learning_rate:learning_rate,self.training: training})\n",
    "    \n",
    "    def evaluate(self, X_input, Y_input, batch_size):\n",
    "        N = X_input.shape[0]\n",
    "            \n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "            \n",
    "        for i in range(0, N, batch_size):\n",
    "            X_batch = X_input[i:i + batch_size]\n",
    "            Y_batch = Y_input[i:i + batch_size]\n",
    "                \n",
    "            feed_dict = {self.X: X_batch, self.Y: Y_batch, self.training: False}\n",
    "                \n",
    "            loss = self.cost\n",
    "            accuracy = self.accuracy\n",
    "                \n",
    "            step_loss, step_acc = self.sess.run([loss, accuracy], feed_dict=feed_dict)\n",
    "                \n",
    "            total_loss += step_loss * X_batch.shape[0]\n",
    "            total_acc += step_acc * X_batch.shape[0]\n",
    "            \n",
    "        total_loss /= N\n",
    "        total_acc /= N\n",
    "            \n",
    "        return total_loss, total_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model4() :\n",
    "    def __init__(self, sess, name):\n",
    "        self.sess = sess\n",
    "        self.name = name\n",
    "  \n",
    "    def build(self) :\n",
    "        with tf.variable_scope(self.name) :\n",
    "            \n",
    "            self.X = tf.placeholder(tf.float32, [None, input_length, input_size])\n",
    "            self.Y = tf.placeholder(tf.float32, [None, input_class])\n",
    "            self.learning_rate =  tf.placeholder(tf.float32)\n",
    "            self.training = tf.placeholder(tf.bool)\n",
    "            \n",
    "            cell1 = tf.nn.rnn_cell.BasicLSTMCell(hidden_layer2)\n",
    "            dropout1 = tf.nn.rnn_cell.DropoutWrapper(cell1, output_keep_prob=0.5)\n",
    "            cell2 = tf.nn.rnn_cell.BasicLSTMCell(hidden_layer2)\n",
    "            dropout2 = tf.nn.rnn_cell.DropoutWrapper(cell2, output_keep_prob=0.5)\n",
    "            cell3 = tf.nn.rnn_cell.BasicLSTMCell(hidden_layer2)\n",
    "            \n",
    "            multi_cell = tf.nn.rnn_cell.MultiRNNCell([dropout1, dropout2, cell3])\n",
    "            \n",
    "            output, state = tf.nn.dynamic_rnn(multi_cell, self.X, dtype=tf.float32)\n",
    "            output = tf.transpose(output,[1,0,2])[-1]\n",
    "            \n",
    "            dense = tf.layers.dense(inputs=output, units=input_class)\n",
    "            self.logits = dense\n",
    "\n",
    "            self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=self.logits, labels=self.Y))\n",
    "            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope=self.name)\n",
    "            \n",
    "            with tf.control_dependencies(update_ops):\n",
    "                self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.cost)\n",
    "\n",
    "            correct_prediction = tf.equal(tf.argmax(self.logits, 1), tf.argmax(self.Y, 1))     \n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    def predict(self, X_input, training=False):\n",
    "        return self.sess.run(self.logits,feed_dict={self.X: X_input, self.training: training})\n",
    "\n",
    "    def get_accuracy(self, X_input, Y_input, training=False):\n",
    "        return self.sess.run(self.accuracy,feed_dict={self.X: X_input,self.Y: Y_input, self.training: training})\n",
    "\n",
    "    def train(self, X_input, Y_input, learning_rate,training=True):\n",
    "        return self.sess.run([self.cost, self.optimizer], feed_dict={self.X: X_input, self.Y: Y_input, self.learning_rate:learning_rate,self.training: training})\n",
    "    \n",
    "    def evaluate(self, X_input, Y_input, batch_size):\n",
    "        N = X_input.shape[0]\n",
    "            \n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "            \n",
    "        for i in range(0, N, batch_size):\n",
    "            X_batch = X_input[i:i + batch_size]\n",
    "            Y_batch = Y_input[i:i + batch_size]\n",
    "                \n",
    "            feed_dict = {self.X: X_batch, self.Y: Y_batch, self.training: False}\n",
    "                \n",
    "            loss = self.cost\n",
    "            accuracy = self.accuracy\n",
    "                \n",
    "            step_loss, step_acc = self.sess.run([loss, accuracy], feed_dict=feed_dict)\n",
    "                \n",
    "            total_loss += step_loss * X_batch.shape[0]\n",
    "            total_acc += step_acc * X_batch.shape[0]\n",
    "            \n",
    "        total_loss /= N\n",
    "        total_acc /= N\n",
    "            \n",
    "        return total_loss, total_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model5() :\n",
    "    def __init__(self, sess, name):\n",
    "        self.sess = sess\n",
    "        self.name = name\n",
    "  \n",
    "    def build(self) :\n",
    "        with tf.variable_scope(self.name) :\n",
    "            \n",
    "            self.X = tf.placeholder(tf.float32, [None, input_length, input_size])\n",
    "            self.Y = tf.placeholder(tf.float32, [None, input_class])\n",
    "            self.learning_rate =  tf.placeholder(tf.float32)\n",
    "            self.training = tf.placeholder(tf.bool)\n",
    "            \n",
    "            cell1 = tf.nn.rnn_cell.BasicLSTMCell(hidden_layer1)\n",
    "            dropout1 = tf.nn.rnn_cell.DropoutWrapper(cell1, output_keep_prob=0.5)\n",
    "            cell2 = tf.nn.rnn_cell.BasicLSTMCell(hidden_layer1)\n",
    "            multi_cell = tf.nn.rnn_cell.MultiRNNCell([dropout1, cell2])\n",
    "            \n",
    "            output, state = tf.nn.dynamic_rnn(multi_cell, self.X, dtype=tf.float32)\n",
    "            output = tf.transpose(output,[1,0,2])[-1]\n",
    "            \n",
    "            dense1= tf.layers.dense(inputs=output, units=32)\n",
    "            dense2 = tf.layers.dense(inputs=dense1, units=input_class)\n",
    "            self.logits = dense2\n",
    "\n",
    "            self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=self.logits, labels=self.Y))\n",
    "            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope=self.name)\n",
    "            \n",
    "            with tf.control_dependencies(update_ops):\n",
    "                self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.cost)\n",
    "\n",
    "            correct_prediction = tf.equal(tf.argmax(self.logits, 1), tf.argmax(self.Y, 1))     \n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    def predict(self, X_input, training=False):\n",
    "        return self.sess.run(self.logits,feed_dict={self.X: X_input, self.training: training})\n",
    "\n",
    "    def get_accuracy(self, X_input, Y_input, training=False):\n",
    "        return self.sess.run(self.accuracy,feed_dict={self.X: X_input,self.Y: Y_input, self.training: training})\n",
    "\n",
    "    def train(self, X_input, Y_input, learning_rate,training=True):\n",
    "        return self.sess.run([self.cost, self.optimizer], feed_dict={self.X: X_input, self.Y: Y_input, self.learning_rate:learning_rate,self.training: training})\n",
    "    \n",
    "    def evaluate(self, X_input, Y_input, batch_size):\n",
    "        N = X_input.shape[0]\n",
    "            \n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "            \n",
    "        for i in range(0, N, batch_size):\n",
    "            X_batch = X_input[i:i + batch_size]\n",
    "            Y_batch = Y_input[i:i + batch_size]\n",
    "                \n",
    "            feed_dict = {self.X: X_batch, self.Y: Y_batch, self.training: False}\n",
    "                \n",
    "            loss = self.cost\n",
    "            accuracy = self.accuracy\n",
    "                \n",
    "            step_loss, step_acc = self.sess.run([loss, accuracy], feed_dict=feed_dict)\n",
    "                \n",
    "            total_loss += step_loss * X_batch.shape[0]\n",
    "            total_acc += step_acc * X_batch.shape[0]\n",
    "            \n",
    "        total_loss /= N\n",
    "        total_acc /= N\n",
    "            \n",
    "        return total_loss, total_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model6() :\n",
    "    def __init__(self, sess, name):\n",
    "        self.sess = sess\n",
    "        self.name = name\n",
    "  \n",
    "    def build(self) :\n",
    "        with tf.variable_scope(self.name) :\n",
    "            \n",
    "            self.X = tf.placeholder(tf.float32, [None, input_length, input_size])\n",
    "            self.Y = tf.placeholder(tf.float32, [None, input_class])\n",
    "            self.learning_rate =  tf.placeholder(tf.float32)\n",
    "            self.training = tf.placeholder(tf.bool)\n",
    "            \n",
    "            cell1 = tf.nn.rnn_cell.BasicLSTMCell(hidden_layer2)\n",
    "            dropout1 = tf.nn.rnn_cell.DropoutWrapper(cell1, output_keep_prob=0.5)\n",
    "            cell2 = tf.nn.rnn_cell.BasicLSTMCell(hidden_layer2)\n",
    "            multi_cell = tf.nn.rnn_cell.MultiRNNCell([dropout1, cell2])\n",
    "            \n",
    "            output, state = tf.nn.dynamic_rnn(multi_cell, self.X, dtype=tf.float32)\n",
    "            output = tf.transpose(output,[1,0,2])[-1]\n",
    "            \n",
    "            dense1= tf.layers.dense(inputs=output, units=64)\n",
    "            dense2 = tf.layers.dense(inputs=dense1, units=input_class)\n",
    "            self.logits = dense2\n",
    "\n",
    "            self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=self.logits, labels=self.Y))\n",
    "            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope=self.name)\n",
    "            \n",
    "            with tf.control_dependencies(update_ops):\n",
    "                self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.cost)\n",
    "\n",
    "            correct_prediction = tf.equal(tf.argmax(self.logits, 1), tf.argmax(self.Y, 1))     \n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    def predict(self, X_input, training=False):\n",
    "        return self.sess.run(self.logits,feed_dict={self.X: X_input, self.training: training})\n",
    "\n",
    "    def get_accuracy(self, X_input, Y_input, training=False):\n",
    "        return self.sess.run(self.accuracy,feed_dict={self.X: X_input,self.Y: Y_input, self.training: training})\n",
    "\n",
    "    def train(self, X_input, Y_input, learning_rate,training=True):\n",
    "        return self.sess.run([self.cost, self.optimizer], feed_dict={self.X: X_input, self.Y: Y_input, self.learning_rate:learning_rate,self.training: training})\n",
    "    \n",
    "    def evaluate(self, X_input, Y_input, batch_size):\n",
    "        N = X_input.shape[0]\n",
    "            \n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "            \n",
    "        for i in range(0, N, batch_size):\n",
    "            X_batch = X_input[i:i + batch_size]\n",
    "            Y_batch = Y_input[i:i + batch_size]\n",
    "                \n",
    "            feed_dict = {self.X: X_batch, self.Y: Y_batch, self.training: False}\n",
    "                \n",
    "            loss = self.cost\n",
    "            accuracy = self.accuracy\n",
    "                \n",
    "            step_loss, step_acc = self.sess.run([loss, accuracy], feed_dict=feed_dict)\n",
    "                \n",
    "            total_loss += step_loss * X_batch.shape[0]\n",
    "            total_acc += step_acc * X_batch.shape[0]\n",
    "            \n",
    "        total_loss /= N\n",
    "        total_acc /= N\n",
    "            \n",
    "        return total_loss, total_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model7() :\n",
    "    def __init__(self, sess, name):\n",
    "        self.sess = sess\n",
    "        self.name = name\n",
    "  \n",
    "    def build(self) :\n",
    "        with tf.variable_scope(self.name) :\n",
    "            \n",
    "            self.X = tf.placeholder(tf.float32, [None, input_length, input_size])\n",
    "            self.Y = tf.placeholder(tf.float32, [None, input_class])\n",
    "            self.learning_rate =  tf.placeholder(tf.float32)\n",
    "            self.training = tf.placeholder(tf.bool)\n",
    "            \n",
    "            cell1 = tf.nn.rnn_cell.BasicLSTMCell(hidden_layer1)\n",
    "            dropout1 = tf.nn.rnn_cell.DropoutWrapper(cell1, output_keep_prob=0.5)\n",
    "            cell2 = tf.nn.rnn_cell.BasicLSTMCell(hidden_layer1)\n",
    "            dropout2 = tf.nn.rnn_cell.DropoutWrapper(cell2, output_keep_prob=0.5)\n",
    "            cell3 = tf.nn.rnn_cell.BasicLSTMCell(hidden_layer1)\n",
    "            \n",
    "            multi_cell = tf.nn.rnn_cell.MultiRNNCell([dropout1, dropout2, cell3])\n",
    "            \n",
    "            output, state = tf.nn.dynamic_rnn(multi_cell, self.X, dtype=tf.float32)\n",
    "            output = tf.transpose(output,[1,0,2])[-1]\n",
    "            \n",
    "            dense1= tf.layers.dense(inputs=output, units=32)\n",
    "            dense2 = tf.layers.dense(inputs=dense1, units=input_class)\n",
    "            self.logits = dense2\n",
    "\n",
    "            self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=self.logits, labels=self.Y))\n",
    "            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope=self.name)\n",
    "            \n",
    "            with tf.control_dependencies(update_ops):\n",
    "                self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.cost)\n",
    "\n",
    "            correct_prediction = tf.equal(tf.argmax(self.logits, 1), tf.argmax(self.Y, 1))     \n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    def predict(self, X_input, training=False):\n",
    "        return self.sess.run(self.logits,feed_dict={self.X: X_input, self.training: training})\n",
    "\n",
    "    def get_accuracy(self, X_input, Y_input, training=False):\n",
    "        return self.sess.run(self.accuracy,feed_dict={self.X: X_input,self.Y: Y_input, self.training: training})\n",
    "\n",
    "    def train(self, X_input, Y_input, learning_rate,training=True):\n",
    "        return self.sess.run([self.cost, self.optimizer], feed_dict={self.X: X_input, self.Y: Y_input, self.learning_rate:learning_rate,self.training: training})\n",
    "    \n",
    "    def evaluate(self, X_input, Y_input, batch_size):\n",
    "        N = X_input.shape[0]\n",
    "            \n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "            \n",
    "        for i in range(0, N, batch_size):\n",
    "            X_batch = X_input[i:i + batch_size]\n",
    "            Y_batch = Y_input[i:i + batch_size]\n",
    "                \n",
    "            feed_dict = {self.X: X_batch, self.Y: Y_batch, self.training: False}\n",
    "                \n",
    "            loss = self.cost\n",
    "            accuracy = self.accuracy\n",
    "                \n",
    "            step_loss, step_acc = self.sess.run([loss, accuracy], feed_dict=feed_dict)\n",
    "                \n",
    "            total_loss += step_loss * X_batch.shape[0]\n",
    "            total_acc += step_acc * X_batch.shape[0]\n",
    "            \n",
    "        total_loss /= N\n",
    "        total_acc /= N\n",
    "            \n",
    "        return total_loss, total_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model8() :\n",
    "    def __init__(self, sess, name):\n",
    "        self.sess = sess\n",
    "        self.name = name\n",
    "  \n",
    "    def build(self) :\n",
    "        with tf.variable_scope(self.name) :\n",
    "            \n",
    "            self.X = tf.placeholder(tf.float32, [None, input_length, input_size])\n",
    "            self.Y = tf.placeholder(tf.float32, [None, input_class])\n",
    "            self.learning_rate =  tf.placeholder(tf.float32)\n",
    "            self.training = tf.placeholder(tf.bool)\n",
    "            \n",
    "            cell1 = tf.nn.rnn_cell.BasicLSTMCell(hidden_layer2)\n",
    "            dropout1 = tf.nn.rnn_cell.DropoutWrapper(cell1, output_keep_prob=0.5)\n",
    "            cell2 = tf.nn.rnn_cell.BasicLSTMCell(hidden_layer2)\n",
    "            dropout2 = tf.nn.rnn_cell.DropoutWrapper(cell2, output_keep_prob=0.5)\n",
    "            cell3 = tf.nn.rnn_cell.BasicLSTMCell(hidden_layer2)\n",
    "            \n",
    "            multi_cell = tf.nn.rnn_cell.MultiRNNCell([dropout1, dropout2, cell3])\n",
    "            \n",
    "            output, state = tf.nn.dynamic_rnn(multi_cell, self.X, dtype=tf.float32)\n",
    "            output = tf.transpose(output,[1,0,2])[-1]\n",
    "            \n",
    "            dense1= tf.layers.dense(inputs=output, units=64)\n",
    "            dense2 = tf.layers.dense(inputs=dense1, units=input_class)\n",
    "            self.logits = dense2\n",
    "\n",
    "            self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=self.logits, labels=self.Y))\n",
    "            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope=self.name)\n",
    "            \n",
    "            with tf.control_dependencies(update_ops):\n",
    "                self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.cost)\n",
    "\n",
    "            correct_prediction = tf.equal(tf.argmax(self.logits, 1), tf.argmax(self.Y, 1))     \n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    def predict(self, X_input, training=False):\n",
    "        return self.sess.run(self.logits,feed_dict={self.X: X_input, self.training: training})\n",
    "\n",
    "    def get_accuracy(self, X_input, Y_input, training=False):\n",
    "        return self.sess.run(self.accuracy,feed_dict={self.X: X_input,self.Y: Y_input, self.training: training})\n",
    "\n",
    "    def train(self, X_input, Y_input, learning_rate,training=True):\n",
    "        return self.sess.run([self.cost, self.optimizer], feed_dict={self.X: X_input, self.Y: Y_input, self.learning_rate:learning_rate,self.training: training})\n",
    "    \n",
    "    def evaluate(self, X_input, Y_input, batch_size):\n",
    "        N = X_input.shape[0]\n",
    "            \n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "            \n",
    "        for i in range(0, N, batch_size):\n",
    "            X_batch = X_input[i:i + batch_size]\n",
    "            Y_batch = Y_input[i:i + batch_size]\n",
    "                \n",
    "            feed_dict = {self.X: X_batch, self.Y: Y_batch, self.training: False}\n",
    "                \n",
    "            loss = self.cost\n",
    "            accuracy = self.accuracy\n",
    "                \n",
    "            step_loss, step_acc = self.sess.run([loss, accuracy], feed_dict=feed_dict)\n",
    "                \n",
    "            total_loss += step_loss * X_batch.shape[0]\n",
    "            total_acc += step_acc * X_batch.shape[0]\n",
    "            \n",
    "        total_loss /= N\n",
    "        total_acc /= N\n",
    "            \n",
    "        return total_loss, total_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br><br></br><br></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate1 = 0.01\n",
    "learning_rate2 = 0.005\n",
    "learning_rate3 = 0.002\n",
    "learning_rate4 = 0.001\n",
    "\n",
    "total_epoch = 160\n",
    "batch_size = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses1 = []\n",
    "train_accs1 = []\n",
    "valid_losses1 = []\n",
    "valid_accs1 = []\n",
    "\n",
    "train_losses2 = []\n",
    "train_accs2 = []\n",
    "valid_losses2 = []\n",
    "valid_accs2 = []\n",
    "\n",
    "train_losses3 = []\n",
    "train_accs3 = []\n",
    "valid_losses3 = []\n",
    "valid_accs3 = []\n",
    "\n",
    "train_losses4 = []\n",
    "train_accs4 = []\n",
    "valid_losses4 = []\n",
    "valid_accs4 = []\n",
    "\n",
    "train_losses5 = []\n",
    "train_accs5 = []\n",
    "valid_losses5 = []\n",
    "valid_accs5 = []\n",
    "\n",
    "train_losses6 = []\n",
    "train_accs6 = []\n",
    "valid_losses6 = []\n",
    "valid_accs6 = []\n",
    "\n",
    "train_losses7 = []\n",
    "train_accs7 = []\n",
    "valid_losses7 = []\n",
    "valid_accs7 = []\n",
    "\n",
    "train_losses8 = []\n",
    "train_accs8 = []\n",
    "valid_losses8 = []\n",
    "valid_accs8 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "model1 = Model1(sess, \"model1\")\n",
    "model1.build()\n",
    "\n",
    "model2 = Model2(sess, \"model2\")\n",
    "model2.build()\n",
    "\n",
    "model3 = Model3(sess, \"model3\")\n",
    "model3.build()\n",
    "\n",
    "model4 = Model4(sess, \"model4\")\n",
    "model4.build()\n",
    "\n",
    "model5 = Model5(sess, \"model5\")\n",
    "model5.build()\n",
    "\n",
    "model6 = Model6(sess, \"model6\")\n",
    "model6.build()\n",
    "\n",
    "model7 = Model7(sess, \"model7\")\n",
    "model7.build()\n",
    "\n",
    "model8 = Model8(sess, \"model8\")\n",
    "model8.build()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Learning Started!')\n",
    "print(\"\")\n",
    "\n",
    "# train my model\n",
    "for epoch in range(total_epoch):\n",
    "    avg_cost1 = 0\n",
    "    avg_cost2 = 0\n",
    "    avg_cost3 = 0\n",
    "    avg_cost4 = 0\n",
    "    avg_cost5 = 0\n",
    "    avg_cost6 = 0\n",
    "    avg_cost7 = 0\n",
    "    avg_cost8 = 0\n",
    "    \n",
    "    total_batch = int(len(training_lst) / batch_size)\n",
    "    idx = 0\n",
    "    \n",
    "    if epoch == 0 :\n",
    "        learning_rate = learning_rate1\n",
    "    elif epoch == 40 :\n",
    "        learning_rate = learning_rate2\n",
    "    elif epoch == 80 :\n",
    "        learning_rate = learning_rate3\n",
    "    elif epoch == 120 :\n",
    "        learning_rate = learning_rate4\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = training_lst[idx:idx+batch_size],training_label[idx:idx+batch_size]\n",
    "        \n",
    "        c1, _ = model1.train(batch_xs, batch_ys, learning_rate)\n",
    "        c2, _ = model2.train(batch_xs, batch_ys, learning_rate)\n",
    "        c3, _ = model3.train(batch_xs, batch_ys, learning_rate)\n",
    "        c4, _ = model4.train(batch_xs, batch_ys, learning_rate)\n",
    "        c5, _ = model5.train(batch_xs, batch_ys, learning_rate)\n",
    "        c6, _ = model6.train(batch_xs, batch_ys, learning_rate)\n",
    "        c7, _ = model7.train(batch_xs, batch_ys, learning_rate)\n",
    "        c8, _ = model8.train(batch_xs, batch_ys, learning_rate)\n",
    "        \n",
    "        avg_cost1 += c1 / total_batch\n",
    "        avg_cost2 += c2 / total_batch\n",
    "        avg_cost3 += c3 / total_batch\n",
    "        avg_cost4 += c4 / total_batch\n",
    "        avg_cost5 += c5 / total_batch\n",
    "        avg_cost6 += c6 / total_batch\n",
    "        avg_cost7 += c7 / total_batch\n",
    "        avg_cost8 += c8 / total_batch\n",
    "        \n",
    "        idx += batch_size\n",
    "        \n",
    "        if i%55 == 0 :\n",
    "            print(\"log1 : \", avg_cost1)\n",
    "            print(\"log2 : \", avg_cost2)\n",
    "            print(\"log3 : \", avg_cost3)\n",
    "            print(\"log4 : \", avg_cost4)\n",
    "            print(\"log5 : \", avg_cost5)\n",
    "            print(\"log6 : \", avg_cost6)\n",
    "            print(\"log7 : \", avg_cost7)\n",
    "            print(\"log8 : \", avg_cost8)\n",
    "            \n",
    "    #train cost & acc\n",
    "    cost1, acc1 = model1.evaluate(training_lst, training_label, batch_size = batch_size)\n",
    "    cost2, acc2 = model2.evaluate(training_lst, training_label, batch_size = batch_size)\n",
    "    cost3, acc3 = model3.evaluate(training_lst, training_label, batch_size = batch_size)\n",
    "    cost4, acc4 = model4.evaluate(training_lst, training_label, batch_size = batch_size)\n",
    "    cost5, acc5 = model5.evaluate(training_lst, training_label, batch_size = batch_size)\n",
    "    cost6, acc6 = model6.evaluate(training_lst, training_label, batch_size = batch_size)\n",
    "    cost7, acc7 = model7.evaluate(training_lst, training_label, batch_size = batch_size)\n",
    "    cost8, acc8 = model8.evaluate(training_lst, training_label, batch_size = batch_size)\n",
    "    \n",
    "    train_losses1.append(cost1)\n",
    "    train_accs1.append(acc1)\n",
    "    train_losses2.append(cost2)\n",
    "    train_accs2.append(acc2)\n",
    "    train_losses3.append(cost3)\n",
    "    train_accs3.append(acc3)\n",
    "    train_losses4.append(cost4)\n",
    "    train_accs4.append(acc4)\n",
    "    train_losses5.append(cost5)\n",
    "    train_accs5.append(acc5)\n",
    "    train_losses6.append(cost6)\n",
    "    train_accs6.append(acc6)\n",
    "    train_losses7.append(cost7)\n",
    "    train_accs7.append(acc7)\n",
    "    train_losses8.append(cost8)\n",
    "    train_accs8.append(acc8)\n",
    "    \n",
    "    #test cost & acc\n",
    "    v_cost1, v_acc1 = model1.evaluate(valid_lst, valid_label, batch_size = batch_size)\n",
    "    v_cost2, v_acc2 = model2.evaluate(valid_lst, valid_label, batch_size = batch_size)\n",
    "    v_cost3, v_acc3 = model3.evaluate(valid_lst, valid_label, batch_size = batch_size)\n",
    "    v_cost4, v_acc4 = model4.evaluate(valid_lst, valid_label, batch_size = batch_size)\n",
    "    v_cost5, v_acc5 = model5.evaluate(valid_lst, valid_label, batch_size = batch_size)\n",
    "    v_cost6, v_acc6 = model6.evaluate(valid_lst, valid_label, batch_size = batch_size)\n",
    "    v_cost7, v_acc7 = model7.evaluate(valid_lst, valid_label, batch_size = batch_size)\n",
    "    v_cost8, v_acc8 = model8.evaluate(valid_lst, valid_label, batch_size = batch_size)\n",
    "    \n",
    "    valid_losses1.append(v_cost1)\n",
    "    valid_accs1.append(v_acc1)\n",
    "    valid_losses2.append(v_cost2)\n",
    "    valid_accs2.append(v_acc2)\n",
    "    valid_losses3.append(v_cost3)\n",
    "    valid_accs3.append(v_acc3)\n",
    "    valid_losses4.append(v_cost4)\n",
    "    valid_accs4.append(v_acc4)\n",
    "    valid_losses5.append(v_cost5)\n",
    "    valid_accs5.append(v_acc5)\n",
    "    valid_losses6.append(v_cost6)\n",
    "    valid_accs6.append(v_acc6)\n",
    "    valid_losses7.append(v_cost7)\n",
    "    valid_accs7.append(v_acc7)\n",
    "    valid_losses8.append(v_cost8)\n",
    "    valid_accs8.append(v_acc8)\n",
    "    \n",
    "    print(\"epoch : \", epoch, \" -- train {:.5f}({:.1f}%), valid{:.5f}({:.1f}%)\".format(cost1, acc1*100, v_cost1, v_acc1*100))\n",
    "    print(\"epoch : \", epoch, \" -- train {:.5f}({:.1f}%), valid{:.5f}({:.1f}%)\".format(cost2, acc2*100, v_cost2, v_acc2*100))\n",
    "    print(\"epoch : \", epoch, \" -- train {:.5f}({:.1f}%), valid{:.5f}({:.1f}%)\".format(cost3, acc3*100, v_cost3, v_acc3*100))\n",
    "    print(\"epoch : \", epoch, \" -- train {:.5f}({:.1f}%), valid{:.5f}({:.1f}%)\".format(cost4, acc4*100, v_cost4, v_acc4*100))\n",
    "    print(\"epoch : \", epoch, \" -- train {:.5f}({:.1f}%), valid{:.5f}({:.1f}%)\".format(cost5, acc5*100, v_cost5, v_acc5*100))\n",
    "    print(\"epoch : \", epoch, \" -- train {:.5f}({:.1f}%), valid{:.5f}({:.1f}%)\".format(cost6, acc6*100, v_cost6, v_acc6*100))\n",
    "    print(\"epoch : \", epoch, \" -- train {:.5f}({:.1f}%), valid{:.5f}({:.1f}%)\".format(cost7, acc7*100, v_cost7, v_acc7*100))\n",
    "    print(\"epoch : \", epoch, \" -- train {:.5f}({:.1f}%), valid{:.5f}({:.1f}%)\".format(cost8, acc8*100, v_cost8, v_acc8*100))\n",
    "    print('Accuracy:', model1.get_accuracy(test_lst, test_label))\n",
    "    print('Accuracy:', model2.get_accuracy(test_lst, test_label))\n",
    "    print('Accuracy:', model3.get_accuracy(test_lst, test_label))\n",
    "    print('Accuracy:', model4.get_accuracy(test_lst, test_label))\n",
    "    print('Accuracy:', model5.get_accuracy(test_lst, test_label))\n",
    "    print('Accuracy:', model6.get_accuracy(test_lst, test_label))\n",
    "    print('Accuracy:', model7.get_accuracy(test_lst, test_label))\n",
    "    print('Accuracy:', model8.get_accuracy(test_lst, test_label))\n",
    "    print(\" \")\n",
    "\n",
    "print(\"\")\n",
    "print('Learning Finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for idx in range(1,9) :\n",
    "    plt.plot(eval(\"train_losses\"+str(idx)), label='training'+str(idx))\n",
    "    plt.plot(eval(\"valid_losses\"+str(idx)), label='valid'+str(idx))\n",
    "    plt.title(\"model\"+str(idx))\n",
    "    plt.grid(\"on\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for idx in range(1,9) :\n",
    "    plt.plot(eval(\"train_accs\"+str(idx)), label='training'+str(idx))\n",
    "    plt.plot(eval(\"valid_accs\"+str(idx)), label='valid'+str(idx))\n",
    "    plt.title(\"model\"+str(idx))\n",
    "    plt.grid(\"on\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(1,9) :\n",
    "    plt.plot(eval(\"train_losses\"+str(idx)), label='training'+str(idx))\n",
    "    \n",
    "plt.grid(\"on\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(1,9) :\n",
    "    plt.plot(eval(\"valid_accs\"+str(idx)), label='valid'+str(idx))\n",
    "    \n",
    "plt.grid(\"on\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br><br></br><br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- payment \n",
    "    - week별 충전 금액\n",
    "    \n",
    "- trade\n",
    "    - week별 교환 신청 횟수\n",
    "    - week별 교환 받은 횟수\n",
    "    - 교환 물품 횟수\n",
    "    \n",
    "- guild\n",
    "    - 있었던 길드의 개수\n",
    "    - 길드원의 평균 activity\n",
    "\n",
    "- party \n",
    "    - week별 party 횟수\n",
    "    - 파티원의 평균 activity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 의문점\n",
    "    - guild가 여러 개인 경우에는 어떻게 처리할건지\n",
    "    - party와 guild의 정보를 어떻게 할건지\n",
    "        - 현재 multi input model 알아보는 중, 지금까지 찾은건 맘에 드는 것이 없음\n",
    "        - party id별, guild id별 정보를 만드는 것이 중요해보임\n",
    "    - 플레이를 했던 시간대도 굉장히 중요할거같은데 이것을 어떻게 처리해야할지 애매"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 모델링 계획\n",
    "    - zero padding으로 해보기(현재는 각 column의 최소값)\n",
    "        - 3주차에 안하는 것과 8주차에 안하는 것은 차이나기 때문에 데이터를 몰아서 넣는 것이 아니라 중간중간에 빈 곳을 padding해줘야 함\n",
    "    - guild/party등의 정보를 초기 state혹은 input에 concatenate하는 방식\n",
    "    - CNN 사용\n",
    "    - AutoEncoder 사용\n",
    "        - guild/party 등의 정보를 추출할 때 사용\n",
    "    - 모든 활동 내역을 시간 순으로 나열하여 각 주별로 RNN으로 처리하여 encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 해결책\n",
    "    - 각 주차에 시작한 사람들끼리 모델을 만들기\n",
    "        - 1주차에 시작한 사람이랑 5주차에 시작한 사람에 차이를 줘야하기 때문\n",
    "    - Attention\n",
    "        - Transformer\n",
    "    - token\n",
    "        - 총 4개의 token을 사용(start, end, empty, zero)\n",
    "        - 비어있는 것을 하나로 합치고, 대신 얼마나 비어있는지를 값으로 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
