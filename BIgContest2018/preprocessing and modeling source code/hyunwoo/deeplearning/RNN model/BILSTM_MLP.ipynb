{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot(lst, num_class=4) :\n",
    "    return np.eye(num_class)[lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def making_data(csv) :\n",
    "    activity = pd.read_csv(csv).drop(\"Unnamed: 0\", axis=1)\n",
    "    label = activity[[\"acc_id\", \"label\"]]\n",
    "    \n",
    "    activity = activity[activity[\"label\"] != \"empty\"]\n",
    "    activity = activity.drop(\"label\", axis=1)\n",
    "    label = label[label[\"label\"] != \"empty\"]\n",
    "    \n",
    "    activity = activity.sort_values([\"acc_id\",\"wk\"])[['acc_id', 'wk', 'cnt_clear_bam', 'cnt_clear_inzone_light','cnt_clear_inzone_normal', \n",
    "                                                  'cnt_clear_inzone_skilled', 'cnt_clear_inzone_solo', 'cnt_clear_raid', 'cnt_clear_raid_light',\n",
    "                                                  'cnt_dt', 'cnt_enter_bam', 'cnt_enter_inzone_light', 'cnt_enter_inzone_normal', \n",
    "                                                  'cnt_enter_inzone_skilled', 'cnt_enter_inzone_solo', 'cnt_enter_raid', 'cnt_enter_raid_light',\n",
    "                                                  'cnt_use_buffitem', 'district_chat', 'duel_cnt', 'duel_win', 'faction_chat', 'game_combat_time', \n",
    "                                                  'gathering_cnt', 'get_money','guild_chat', 'item_hongmun', 'making_cnt', 'normal_chat', \n",
    "                                                  'npc_exp', 'npc_hongmun', 'party_chat', 'partybattle_cnt', 'partybattle_win', 'play_time', \n",
    "                                                  'quest_exp', 'quest_hongmun', 'whisper_chat','first_week', 'payment_amount']]\n",
    "    label = label.sort_values(\"acc_id\")\n",
    "    \n",
    "    label_lst = sorted(list(set([tuple(x) for x in label.values])))\n",
    "    label = pd.DataFrame(label_lst, columns = [\"acc_id\", \"label\"])\n",
    "    \n",
    "    activity1 = activity[activity[\"wk\"]==1].drop([\"acc_id\", \"wk\"], axis=1)\n",
    "    activity2 = activity[activity[\"wk\"]==2].drop([\"acc_id\", \"wk\"], axis=1)\n",
    "    activity3 = activity[activity[\"wk\"]==3].drop([\"acc_id\", \"wk\"], axis=1)\n",
    "    activity4 = activity[activity[\"wk\"]==4].drop([\"acc_id\", \"wk\"], axis=1)\n",
    "    activity5 = activity[activity[\"wk\"]==5].drop([\"acc_id\", \"wk\"], axis=1)\n",
    "    activity6 = activity[activity[\"wk\"]==6].drop([\"acc_id\", \"wk\"], axis=1)\n",
    "    activity7 = activity[activity[\"wk\"]==7].drop([\"acc_id\", \"wk\"], axis=1)\n",
    "    activity8 = activity[activity[\"wk\"]==8].drop([\"acc_id\", \"wk\"], axis=1)\n",
    "    \n",
    "    num_values = len(activity1.values[0])\n",
    "    \n",
    "    activity = np.concatenate([activity1.values.reshape([-1, 1, num_values]), activity2.values.reshape([-1, 1, num_values]), \n",
    "                               activity3.values.reshape([-1, 1, num_values]), activity4.values.reshape([-1, 1, num_values]),\n",
    "                               activity5.values.reshape([-1, 1, num_values]), activity6.values.reshape([-1, 1, num_values]),\n",
    "                               activity7.values.reshape([-1, 1, num_values]), activity8.values.reshape([-1, 1, num_values])], axis=1)\n",
    "    \n",
    "    label_dic = {\"week\":0 , \"month\" :1, \"2month\":2, \"retained\":3}\n",
    "\n",
    "    label2 = label.sort_values(by=\"acc_id\")\n",
    "    label2[\"label\"] = label2[\"label\"].map(lambda x : label_dic[x])\n",
    "    \n",
    "    total_lst = activity\n",
    "    label_dic = label2.label.tolist()\n",
    "    total_label = one_hot(label_dic)\n",
    "    \n",
    "    return total_lst, total_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <br></br><br></br><br></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_lst, total_label = making_data(\"OnlyExpanded.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx1 = len(total_lst)//5 *4\n",
    "\n",
    "training_lst = np.array(total_lst[:idx1])\n",
    "valid_lst = np.array(total_lst[idx1:])\n",
    "\n",
    "training_label = np.array(total_label[:idx1])\n",
    "valid_label = np.array(total_label[idx1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 8, 38)\n",
      "(100000, 4)\n"
     ]
    }
   ],
   "source": [
    "print(np.array(total_lst).shape)\n",
    "print(np.array(total_label).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br><br></br><br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNN() :\n",
    "    def __init__(self, sess, name):\n",
    "        self.sess = sess\n",
    "        self.name = name\n",
    "        \n",
    "    def build(self, batch_size, length, dim, is_embedding, emb_width, num_unit, is_fc, fc_num_unit, fc_activation, cost_function, output_dim) :\n",
    "        with tf.variable_scope(self.name) :\n",
    "            \n",
    "            ## Setting ##\n",
    "            self.batch_size = batch_size\n",
    "            self.length = length\n",
    "            self.dim = dim\n",
    "            self.is_embedding = is_embedding\n",
    "            self.emb_width = emb_width\n",
    "            self.num_unit = num_unit\n",
    "            self.is_fc = is_fc\n",
    "            self.fc_num_unit = fc_num_unit\n",
    "            self.fc_activation = fc_activation\n",
    "            self.output_dim = output_dim\n",
    "            \n",
    "            self.X = tf.placeholder(tf.float32, [self.batch_size, self.length, self.dim])\n",
    "            self.Y = tf.placeholder(tf.float32, [self.batch_size, self.output_dim])\n",
    "            self.learning_rate =  tf.placeholder(tf.float32)\n",
    "            self.training = tf.placeholder(tf.bool)\n",
    "            #############\n",
    "            \n",
    "            \n",
    "            ## Embedding ##\n",
    "            if self.is_embedding :\n",
    "                W_emb = tf.Variable(tf.random_normal([self.width, self.emb_width]))\n",
    "                self.X = tf.concat(self.X, tf.matmul(self.X, W_emb), axis=2)\n",
    "            ###############\n",
    "            \n",
    "            \n",
    "            ## RNN ##\n",
    "            f_cell1 = tf.nn.rnn_cell.BasicLSTMCell(self.num_unit)\n",
    "            f_cell2 = tf.nn.rnn_cell.BasicLSTMCell(self.num_unit)\n",
    "            f_multi_cell = tf.nn.rnn_cell.MultiRNNCell([f_cell1, f_cell2])\n",
    "            \n",
    "            b_cell1 = tf.nn.rnn_cell.BasicLSTMCell(self.num_unit)\n",
    "            b_cell2 = tf.nn.rnn_cell.BasicLSTMCell(self.num_unit)\n",
    "            b_multi_cell = tf.nn.rnn_cell.MultiRNNCell([b_cell1, b_cell2])\n",
    "            \n",
    "            f_output, f_state = tf.nn.dynamic_rnn(f_multi_cell, self.X, dtype=tf.float32,  scope=\"forward\")\n",
    "            b_output, b_state = tf.nn.dynamic_rnn(b_multi_cell, tf.reverse(self.X, axis=[1]), dtype=tf.float32, scope=\"backward\")\n",
    "\n",
    "            hidden1 = tf.concat([f_output[:,0,:], b_output[:,0,:]], axis=1)\n",
    "            hidden2 = tf.concat([f_output[:,1,:], b_output[:,1,:]], axis=1)\n",
    "            hidden3 = tf.concat([f_output[:,2,:], b_output[:,2,:]], axis=1)\n",
    "            hidden4 = tf.concat([f_output[:,3,:], b_output[:,3,:]], axis=1)\n",
    "            hidden5 = tf.concat([f_output[:,4,:], b_output[:,4,:]], axis=1)\n",
    "            hidden6 = tf.concat([f_output[:,5,:], b_output[:,5,:]], axis=1)\n",
    "            hidden7 = tf.concat([f_output[:,6,:], b_output[:,6,:]], axis=1)\n",
    "            hidden8 = tf.concat([f_output[:,7,:], b_output[:,7,:]], axis=1)\n",
    "            \n",
    "            dense1 = tf.nn.relu(tf.contrib.layers.layer_norm(tf.layers.dense(hidden1, 32)))\n",
    "            dense2 = tf.nn.relu(tf.contrib.layers.layer_norm(tf.layers.dense(hidden2, 32)))\n",
    "            dense3 = tf.nn.relu(tf.contrib.layers.layer_norm(tf.layers.dense(hidden3, 32)))\n",
    "            dense4 = tf.nn.relu(tf.contrib.layers.layer_norm(tf.layers.dense(hidden4, 32)))\n",
    "            dense5 = tf.nn.relu(tf.contrib.layers.layer_norm(tf.layers.dense(hidden5, 32)))\n",
    "            dense6 = tf.nn.relu(tf.contrib.layers.layer_norm(tf.layers.dense(hidden6, 32)))\n",
    "            dense7 = tf.nn.relu(tf.contrib.layers.layer_norm(tf.layers.dense(hidden7, 32)))\n",
    "            dense8 = tf.nn.relu(tf.contrib.layers.layer_norm(tf.layers.dense(hidden8, 32)))\n",
    "            \n",
    "            concat = tf.concat([hidden1, hidden2, hidden3, hidden4, hidden5, hidden6, hidden7, hidden8], axis=1)\n",
    "            rnn_result = tf.layers.dense(concat, 4)\n",
    "            #########\n",
    "            \n",
    "            \n",
    "            ## Classifier ##\n",
    "            if is_fc : \n",
    "                dense= tf.layers.dense(rnn_result, self.fc_num_unit)\n",
    "                norm = tf.contrib.layers.layer_norm(dense)\n",
    "                relu = tf.nn.relu(norm)\n",
    "                self.logit = tf.layers.dense(norm, 4)\n",
    "            else :\n",
    "                self.logit = tf.layers.dense(rnn_result, 4)\n",
    "                \n",
    "            self.softmax = tf.nn.softmax(self.logit)\n",
    "            ################\n",
    "            \n",
    "            \n",
    "            ## Learning ##\n",
    "            if cost_function == \"f1\" :\n",
    "                self.numerator = tf.reduce_sum(self.softmax*self.Y)\n",
    "                self.denominator = tf.reduce_sum(self.softmax*self.Y + self.Y)\n",
    "                self.cost = -2 * self.numerator / self.denominator\n",
    "                \n",
    "            else :\n",
    "                self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=self.logit, labels=self.Y))\n",
    "\n",
    "            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope=self.name)\n",
    "            with tf.control_dependencies(update_ops):\n",
    "                self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.cost)\n",
    "            \n",
    "            self.prediction = tf.equal(tf.argmax(self.logit, 1), tf.argmax(self.Y, 1))     \n",
    "            self.accuracy = tf.reduce_mean(tf.cast(self.prediction, tf.float32))    \n",
    "            ##############\n",
    "        \n",
    "        \n",
    "    def train(self, X_input, Y_input, learning_rate, training=True):\n",
    "        feed_dict = {self.X: X_input, self.Y: Y_input, self.learning_rate: learning_rate, self.training: training}\n",
    "        _, cost = self.sess.run([self.optimizer, self.cost], feed_dict=feed_dict)\n",
    "        \n",
    "        return _, cost\n",
    "    \n",
    "    def predict(self, X_input, training=False):\n",
    "        feed_dict = {self.X: X_input, self.training: training}\n",
    "        result = self.sess.run([self.logit], feed_dict=feed_dict)\n",
    "            \n",
    "        return result\n",
    "    \n",
    "    def evaluate(self, X_input, Y_input):\n",
    "        size = X_input.shape[0]\n",
    "            \n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "            \n",
    "        for idx in range(0, size, self.batch_size):\n",
    "            X_batch = X_input[idx:idx + batch_size]\n",
    "            Y_batch = Y_input[idx:idx + batch_size]\n",
    "            feed_dict = {self.X: X_batch, self.Y: Y_batch, self.training: False}\n",
    "                \n",
    "            loss = self.cost\n",
    "            accuracy = self.accuracy\n",
    "                \n",
    "            step_loss, step_acc = self.sess.run([loss, accuracy], feed_dict=feed_dict)\n",
    "                \n",
    "            total_loss += step_loss * X_batch.shape[0]\n",
    "            total_acc += step_acc * X_batch.shape[0]\n",
    "            \n",
    "        total_loss /= size\n",
    "        total_acc /= size\n",
    "            \n",
    "        return total_loss, total_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br><br></br><br></br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate1 = 0.02\n",
    "learning_rate2 = 0.01\n",
    "learning_rate3 = 0.005\n",
    "learning_rate4 = 0.001\n",
    "\n",
    "total_epoch = 80\n",
    "batch_size = 500\n",
    "input_dim = np.array(total_lst).shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "Ready!\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "is_pass = False\n",
    "model_lst = []\n",
    "\n",
    "for is_embedding in [False, True] :\n",
    "    for emb_width in [64,128] :\n",
    "        if not is_pass :\n",
    "            is_pass = True\n",
    "            continue\n",
    "            \n",
    "        for num_unit in [128, 256, 512] :\n",
    "            for is_fc in [False, True] :\n",
    "                for cost in [\"accuracy\"] :\n",
    "                    print(idx) \n",
    "                    sess = tf.Session()\n",
    "                    model = RNN(sess, \"model{}\".format(idx))\n",
    "                    model.build(500, 8, 38, False, emb_width, num_unit, is_fc, 128, tf.nn.relu, cost, 4)\n",
    "                    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "                    model_lst.append(model)\n",
    "                    idx +=1\n",
    "            \n",
    "tl_ta_vl_va_lst = [[[],[],[],[]]]*len(model_lst)\n",
    "print(\"Ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Started!\n",
      "\n",
      "log : 0\n",
      "log : 10\n",
      "log : 20\n",
      "log : 30\n",
      "log : 40\n",
      "log : 50\n",
      "log : 60\n",
      "log : 70\n",
      "log : 80\n",
      "log : 90\n",
      "log : 100\n",
      "log : 110\n",
      "log : 120\n",
      "log : 130\n",
      "log : 140\n",
      "log : 150\n",
      "***epoch*** :  0\n",
      "-- train 0.83059(63.3%), valid0.82600(63.6%)\n",
      "-- train 0.90061(58.2%), valid0.89339(58.3%)\n",
      "-- train 0.87576(62.0%), valid0.86860(62.1%)\n",
      "-- train 0.93798(56.0%), valid0.93427(56.4%)\n",
      "-- train 0.96522(59.2%), valid0.95700(59.4%)\n",
      "-- train 0.88608(62.1%), valid0.88272(62.3%)\n",
      "-- train 0.83614(63.3%), valid0.83471(63.4%)\n",
      "-- train 0.86483(61.5%), valid0.86169(62.1%)\n",
      "-- train 0.92983(59.4%), valid0.92369(59.9%)\n",
      "-- train 0.98180(56.8%), valid0.97637(56.9%)\n",
      "-- train 0.92701(58.9%), valid0.91956(58.9%)\n",
      "-- train 1.11252(52.2%), valid1.10198(52.2%)\n",
      "-- train 0.80339(64.6%), valid0.80099(64.8%)\n",
      "-- train 0.98844(57.8%), valid0.97931(57.8%)\n",
      "-- train 0.86336(62.7%), valid0.85998(63.0%)\n",
      "-- train 0.84767(63.0%), valid0.84063(63.4%)\n",
      "-- train 0.97263(56.0%), valid0.97379(56.0%)\n",
      "-- train 0.90883(59.6%), valid0.90092(59.6%)\n",
      "Accuracy: 0.636449997127\n",
      "Accuracy: 0.582699999213\n",
      "Accuracy: 0.621099998057\n",
      "Accuracy: 0.564449997246\n",
      "Accuracy: 0.594450001419\n",
      "Accuracy: 0.622699999809\n",
      "Accuracy: 0.634199999273\n",
      "Accuracy: 0.620700000226\n",
      "Accuracy: 0.598999997973\n",
      "Accuracy: 0.568900001049\n",
      "Accuracy: 0.589449997246\n",
      "Accuracy: 0.522149995714\n",
      "Accuracy: 0.648149998486\n",
      "Accuracy: 0.578450004756\n",
      "Accuracy: 0.629500003159\n",
      "Accuracy: 0.633500002325\n",
      "Accuracy: 0.560499997437\n",
      "Accuracy: 0.596449999511\n",
      " \n",
      "log : 0\n",
      "log : 10\n",
      "log : 20\n",
      "log : 30\n",
      "log : 40\n",
      "log : 50\n",
      "log : 60\n",
      "log : 70\n",
      "log : 80\n",
      "log : 90\n",
      "log : 100\n",
      "log : 110\n",
      "log : 120\n",
      "log : 130\n",
      "log : 140\n",
      "log : 150\n",
      "***epoch*** :  1\n",
      "-- train 0.78029(65.7%), valid0.78062(65.7%)\n",
      "-- train 0.82454(63.1%), valid0.81730(63.3%)\n",
      "-- train 0.80984(64.2%), valid0.80846(64.4%)\n",
      "-- train 0.84373(63.0%), valid0.84227(63.0%)\n",
      "-- train 0.84823(63.4%), valid0.85061(63.3%)\n",
      "-- train 0.86460(60.9%), valid0.86451(61.1%)\n",
      "-- train 0.75065(67.2%), valid0.75507(67.4%)\n",
      "-- train 0.77592(66.3%), valid0.77285(66.6%)\n",
      "-- train 0.78788(65.6%), valid0.78965(65.6%)\n",
      "-- train 0.83956(62.7%), valid0.83660(62.8%)\n",
      "-- train 0.82263(64.5%), valid0.82069(64.5%)\n",
      "-- train 0.87603(62.8%), valid0.87453(62.8%)\n",
      "-- train 0.75014(66.8%), valid0.75085(66.9%)\n",
      "-- train 0.83311(63.1%), valid0.83084(63.2%)\n",
      "-- train 0.77518(65.1%), valid0.77533(65.0%)\n",
      "-- train 0.78841(65.8%), valid0.78888(65.9%)\n",
      "-- train 0.87735(61.6%), valid0.87132(61.9%)\n",
      "-- train 0.82661(63.4%), valid0.82749(63.5%)\n",
      "Accuracy: 0.65664999783\n",
      "Accuracy: 0.633250001073\n",
      "Accuracy: 0.643599997461\n",
      "Accuracy: 0.630250000954\n",
      "Accuracy: 0.632899999619\n",
      "Accuracy: 0.611049997807\n",
      "Accuracy: 0.674249999225\n",
      "Accuracy: 0.665600001812\n",
      "Accuracy: 0.655900001526\n",
      "Accuracy: 0.628149998188\n",
      "Accuracy: 0.64525000453\n",
      "Accuracy: 0.627800002694\n",
      "Accuracy: 0.669149999321\n",
      "Accuracy: 0.631549997628\n",
      "Accuracy: 0.650099997222\n",
      "Accuracy: 0.659499998391\n",
      "Accuracy: 0.619000004232\n",
      "Accuracy: 0.634800001979\n",
      " \n",
      "log : 0\n",
      "log : 10\n",
      "log : 20\n",
      "log : 30\n",
      "log : 40\n",
      "log : 50\n",
      "log : 60\n",
      "log : 70\n",
      "log : 80\n",
      "log : 90\n",
      "log : 100\n",
      "log : 110\n",
      "log : 120\n",
      "log : 130\n",
      "log : 140\n",
      "log : 150\n",
      "***epoch*** :  2\n",
      "-- train 0.74960(67.1%), valid0.75655(66.7%)\n",
      "-- train 0.76018(66.5%), valid0.76561(66.4%)\n",
      "-- train 0.76655(66.6%), valid0.76825(66.6%)\n",
      "-- train 0.82909(63.2%), valid0.83017(63.4%)\n",
      "-- train 0.86166(62.3%), valid0.86307(62.2%)\n",
      "-- train 0.82867(63.6%), valid0.83126(63.5%)\n",
      "-- train 0.73254(67.7%), valid0.74144(67.5%)\n",
      "-- train 0.74364(67.0%), valid0.74721(67.0%)\n",
      "-- train 0.74304(67.3%), valid0.74986(67.1%)\n",
      "-- train 0.79226(64.5%), valid0.79380(64.4%)\n",
      "-- train 0.80023(64.7%), valid0.80380(64.6%)\n",
      "-- train 0.81501(64.6%), valid0.81903(64.6%)\n",
      "-- train 0.72784(68.2%), valid0.73614(67.8%)\n",
      "-- train 0.79400(64.3%), valid0.79560(64.4%)\n",
      "-- train 0.74791(66.6%), valid0.75842(66.3%)\n",
      "-- train 0.77214(65.5%), valid0.77330(65.6%)\n",
      "-- train 0.83952(63.0%), valid0.83772(63.2%)\n",
      "-- train 0.78751(65.0%), valid0.79121(64.7%)\n",
      "Accuracy: 0.666549997032\n",
      "Accuracy: 0.664099997282\n",
      "Accuracy: 0.665600000322\n",
      "Accuracy: 0.633799995482\n",
      "Accuracy: 0.621999999881\n",
      "Accuracy: 0.634500002861\n",
      "Accuracy: 0.674599997699\n",
      "Accuracy: 0.669849997759\n",
      "Accuracy: 0.670949995518\n",
      "Accuracy: 0.644049997628\n",
      "Accuracy: 0.645699998736\n",
      "Accuracy: 0.646450001001\n",
      "Accuracy: 0.678399996459\n",
      "Accuracy: 0.643649999797\n",
      "Accuracy: 0.663300001621\n",
      "Accuracy: 0.655600002408\n",
      "Accuracy: 0.631850004196\n",
      "Accuracy: 0.647350002825\n",
      " \n",
      "log : 0\n",
      "log : 10\n",
      "log : 20\n",
      "log : 30\n",
      "log : 40\n",
      "log : 50\n",
      "log : 60\n",
      "log : 70\n",
      "log : 80\n",
      "log : 90\n",
      "log : 100\n",
      "log : 110\n",
      "log : 120\n",
      "log : 130\n",
      "log : 140\n",
      "log : 150\n",
      "***epoch*** :  3\n",
      "-- train 0.72509(68.1%), valid0.73792(67.6%)\n",
      "-- train 0.73282(67.8%), valid0.74155(67.7%)\n",
      "-- train 0.74420(66.9%), valid0.75426(66.5%)\n",
      "-- train 0.78451(65.9%), valid0.79072(65.5%)\n",
      "-- train 0.80437(65.1%), valid0.81090(65.1%)\n",
      "-- train 0.79151(64.6%), valid0.79769(64.2%)\n",
      "-- train 0.71079(68.7%), valid0.72069(68.3%)\n",
      "-- train 0.73303(67.7%), valid0.73948(67.6%)\n",
      "-- train 0.72042(68.0%), valid0.73490(67.5%)\n",
      "-- train 0.77740(65.1%), valid0.78332(65.0%)\n",
      "-- train 0.78977(64.9%), valid0.79514(64.7%)\n",
      "-- train 0.80344(65.1%), valid0.80803(65.1%)\n",
      "-- train 0.71577(68.4%), valid0.72591(67.9%)\n",
      "-- train 0.75584(66.6%), valid0.76380(66.5%)\n",
      "-- train 0.71922(68.0%), valid0.73285(67.9%)\n",
      "-- train 0.73492(68.0%), valid0.74110(67.8%)\n",
      "-- train 0.77975(66.1%), valid0.78274(66.2%)\n",
      "-- train 0.78111(64.6%), valid0.78675(64.2%)\n",
      "Accuracy: 0.67634999752\n",
      "Accuracy: 0.677199998498\n",
      "Accuracy: 0.664849998057\n",
      "Accuracy: 0.655250003934\n",
      "Accuracy: 0.650699996948\n",
      "Accuracy: 0.642499999702\n",
      "Accuracy: 0.683450001478\n",
      "Accuracy: 0.676350003481\n",
      "Accuracy: 0.675249998271\n",
      "Accuracy: 0.649699999392\n",
      "Accuracy: 0.647249998152\n",
      "Accuracy: 0.65055000037\n",
      "Accuracy: 0.678699998558\n",
      "Accuracy: 0.664749996364\n",
      "Accuracy: 0.678899997473\n",
      "Accuracy: 0.678099998832\n",
      "Accuracy: 0.661699995399\n",
      "Accuracy: 0.642049996555\n",
      " \n",
      "log : 0\n",
      "log : 10\n",
      "log : 20\n",
      "log : 30\n",
      "log : 40\n",
      "log : 50\n",
      "log : 60\n",
      "log : 70\n",
      "log : 80\n",
      "log : 90\n",
      "log : 100\n",
      "log : 110\n",
      "log : 120\n",
      "log : 130\n",
      "log : 140\n",
      "log : 150\n",
      "***epoch*** :  4\n",
      "-- train 0.71735(68.1%), valid0.73567(67.9%)\n",
      "-- train 0.71728(68.3%), valid0.72874(68.1%)\n",
      "-- train 0.72335(67.7%), valid0.73668(67.2%)\n",
      "-- train 0.76199(66.8%), valid0.77183(66.6%)\n",
      "-- train 0.78914(65.0%), valid0.80207(64.3%)\n",
      "-- train 0.78082(65.0%), valid0.78966(64.7%)\n",
      "-- train 0.70289(68.6%), valid0.71963(68.0%)\n",
      "-- train 0.71097(68.6%), valid0.72148(68.4%)\n",
      "-- train 0.72482(68.0%), valid0.74396(67.3%)\n",
      "-- train 0.74633(67.1%), valid0.75551(66.9%)\n",
      "-- train 0.78038(65.1%), valid0.78732(64.7%)\n",
      "-- train 0.78249(66.1%), valid0.79094(65.8%)\n",
      "-- train 0.70163(68.8%), valid0.71855(68.0%)\n",
      "-- train 0.72613(67.8%), valid0.73834(67.6%)\n",
      "-- train 0.70654(68.7%), valid0.72724(68.3%)\n",
      "-- train 0.71239(68.1%), valid0.72217(68.0%)\n",
      "-- train 0.77038(66.1%), valid0.77671(66.0%)\n",
      "-- train 0.76834(66.4%), valid0.77698(66.2%)\n",
      "Accuracy: 0.67879999727\n",
      "Accuracy: 0.681149996817\n",
      "Accuracy: 0.671500000358\n",
      "Accuracy: 0.665850000083\n",
      "Accuracy: 0.643499997258\n",
      "Accuracy: 0.64715000093\n",
      "Accuracy: 0.680399999022\n",
      "Accuracy: 0.683550000191\n",
      "Accuracy: 0.673449997604\n",
      "Accuracy: 0.66874999553\n",
      "Accuracy: 0.647450004518\n",
      "Accuracy: 0.658400000632\n",
      "Accuracy: 0.680400000513\n",
      "Accuracy: 0.676399998367\n",
      "Accuracy: 0.682850000262\n",
      "Accuracy: 0.679849998653\n",
      "Accuracy: 0.659849990904\n",
      "Accuracy: 0.661850000918\n",
      " \n",
      "log : 0\n",
      "log : 10\n",
      "log : 20\n",
      "log : 30\n",
      "log : 40\n",
      "log : 50\n",
      "log : 60\n",
      "log : 70\n",
      "log : 80\n",
      "log : 90\n",
      "log : 100\n",
      "log : 110\n",
      "log : 120\n",
      "log : 130\n",
      "log : 140\n",
      "log : 150\n",
      "***epoch*** :  5\n",
      "-- train 0.70017(68.7%), valid0.72135(68.0%)\n",
      "-- train 0.71809(67.9%), valid0.72850(67.8%)\n",
      "-- train 0.71882(68.4%), valid0.73486(67.8%)\n",
      "-- train 0.74705(66.7%), valid0.75925(66.4%)\n",
      "-- train 0.79519(64.2%), valid0.80839(64.0%)\n",
      "-- train 0.78033(65.0%), valid0.79091(64.7%)\n",
      "-- train 0.69149(69.4%), valid0.71524(68.6%)\n",
      "-- train 0.70207(69.0%), valid0.71885(68.2%)\n",
      "-- train 0.72663(68.6%), valid0.74928(68.1%)\n",
      "-- train 0.72597(68.1%), valid0.74085(67.7%)\n",
      "-- train 0.75690(66.8%), valid0.76609(66.6%)\n",
      "-- train 0.76973(66.1%), valid0.78011(65.7%)\n",
      "-- train 0.68777(69.3%), valid0.70967(68.5%)\n",
      "-- train 0.72282(67.8%), valid0.73712(67.4%)\n",
      "-- train 0.69700(69.0%), valid0.71847(68.5%)\n",
      "-- train 0.70721(68.5%), valid0.71940(68.2%)\n",
      "-- train 0.76255(66.4%), valid0.77204(66.0%)\n",
      "-- train 0.75710(66.6%), valid0.76720(66.4%)\n",
      "Accuracy: 0.680399996042\n",
      "Accuracy: 0.677699998021\n",
      "Accuracy: 0.678399996459\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.663750000298\n",
      "Accuracy: 0.639650000632\n",
      "Accuracy: 0.646850001812\n",
      "Accuracy: 0.686149996519\n",
      "Accuracy: 0.682250000536\n",
      "Accuracy: 0.680799995363\n",
      "Accuracy: 0.676650004089\n",
      "Accuracy: 0.665649998188\n",
      "Accuracy: 0.657450000942\n",
      "Accuracy: 0.685299998522\n",
      "Accuracy: 0.673949998617\n",
      "Accuracy: 0.684699997306\n",
      "Accuracy: 0.681849998236\n",
      "Accuracy: 0.659700003266\n",
      "Accuracy: 0.663850000501\n",
      " \n",
      "log : 0\n",
      "log : 10\n",
      "log : 20\n",
      "log : 30\n",
      "log : 40\n",
      "log : 50\n",
      "log : 60\n",
      "log : 70\n",
      "log : 80\n",
      "log : 90\n",
      "log : 100\n",
      "log : 110\n",
      "log : 120\n",
      "log : 130\n",
      "log : 140\n",
      "log : 150\n",
      "***epoch*** :  6\n",
      "-- train 0.68867(69.3%), valid0.71316(68.6%)\n",
      "-- train 0.69927(69.1%), valid0.71289(68.7%)\n",
      "-- train 0.71539(68.1%), valid0.73401(67.6%)\n",
      "-- train 0.73555(67.4%), valid0.75082(67.0%)\n",
      "-- train 0.77537(66.2%), valid0.78946(65.9%)\n",
      "-- train 0.76009(66.2%), valid0.77447(65.9%)\n",
      "-- train 0.68166(69.8%), valid0.71374(68.7%)\n",
      "-- train 0.69232(69.2%), valid0.71192(68.2%)\n",
      "-- train 0.70003(69.5%), valid0.73098(68.6%)\n",
      "-- train 0.70412(68.7%), valid0.71997(68.4%)\n",
      "-- train 0.74567(67.2%), valid0.75501(67.1%)\n",
      "-- train 0.76027(66.2%), valid0.77278(65.4%)\n",
      "-- train 0.68125(69.8%), valid0.70997(68.8%)\n",
      "-- train 0.70926(68.3%), valid0.72483(67.8%)\n",
      "-- train 0.68735(69.5%), valid0.71165(68.6%)\n",
      "-- train 0.70298(68.1%), valid0.71697(67.7%)\n",
      "-- train 0.76738(65.1%), valid0.77934(64.6%)\n",
      "-- train 0.75779(65.7%), valid0.77089(65.1%)\n",
      "Accuracy: 0.685500000417\n",
      "Accuracy: 0.687199997902\n",
      "Accuracy: 0.675549997389\n",
      "Accuracy: 0.670499998331\n",
      "Accuracy: 0.659449999034\n",
      "Accuracy: 0.659049999714\n",
      "Accuracy: 0.68694999516\n",
      "Accuracy: 0.681599999964\n",
      "Accuracy: 0.686249995232\n",
      "Accuracy: 0.683649994433\n",
      "Accuracy: 0.670500001311\n",
      "Accuracy: 0.654150003195\n",
      "Accuracy: 0.687649999559\n",
      "Accuracy: 0.677699996531\n",
      "Accuracy: 0.68599999845\n",
      "Accuracy: 0.677049998939\n",
      "Accuracy: 0.646300002933\n",
      "Accuracy: 0.651349999011\n",
      " \n",
      "log : 0\n",
      "log : 10\n",
      "log : 20\n",
      "log : 30\n",
      "log : 40\n",
      "log : 50\n",
      "log : 60\n",
      "log : 70\n",
      "log : 80\n",
      "log : 90\n",
      "log : 100\n",
      "log : 110\n",
      "log : 120\n",
      "log : 130\n",
      "log : 140\n",
      "log : 150\n",
      "***epoch*** :  7\n",
      "-- train 0.67748(69.7%), valid0.70947(68.8%)\n",
      "-- train 0.69002(69.2%), valid0.71409(68.5%)\n",
      "-- train 0.69756(68.9%), valid0.71645(68.5%)\n",
      "-- train 0.72259(68.1%), valid0.73696(67.7%)\n",
      "-- train 0.78076(65.5%), valid0.79771(65.1%)\n",
      "-- train 0.75488(66.1%), valid0.77084(65.6%)\n",
      "-- train 0.67984(70.1%), valid0.71316(68.9%)\n",
      "-- train 0.69849(69.4%), valid0.71938(68.5%)\n",
      "-- train 0.68798(69.5%), valid0.72314(68.3%)\n",
      "-- train 0.70196(68.4%), valid0.72156(67.8%)\n",
      "-- train 0.73268(67.7%), valid0.74811(67.4%)\n",
      "-- train 0.74940(66.9%), valid0.76590(66.3%)\n",
      "-- train 0.67774(69.7%), valid0.71107(68.5%)\n",
      "-- train 0.70028(68.8%), valid0.71857(68.0%)\n",
      "-- train 0.68314(69.5%), valid0.71317(68.5%)\n",
      "-- train 0.69406(69.1%), valid0.71166(68.4%)\n",
      "-- train 0.74079(67.1%), valid0.75627(66.7%)\n",
      "-- train 0.74083(66.7%), valid0.75603(66.4%)\n",
      "Accuracy: 0.687549994886\n",
      "Accuracy: 0.684899996221\n",
      "Accuracy: 0.684599994123\n",
      "Accuracy: 0.677350001037\n",
      "Accuracy: 0.650650002062\n",
      "Accuracy: 0.65625\n",
      "Accuracy: 0.68900000006\n",
      "Accuracy: 0.685349997878\n",
      "Accuracy: 0.683300001919\n",
      "Accuracy: 0.678099995852\n",
      "Accuracy: 0.673549994826\n",
      "Accuracy: 0.662700001895\n",
      "Accuracy: 0.685149995983\n",
      "Accuracy: 0.680449993908\n",
      "Accuracy: 0.684899996221\n",
      "Accuracy: 0.684300000966\n",
      "Accuracy: 0.666899999976\n",
      "Accuracy: 0.664349998534\n",
      " \n",
      "log : 0\n",
      "log : 10\n",
      "log : 20\n",
      "log : 30\n",
      "log : 40\n",
      "log : 50\n",
      "log : 60\n",
      "log : 70\n",
      "log : 80\n",
      "log : 90\n",
      "log : 100\n",
      "log : 110\n",
      "log : 120\n",
      "log : 130\n",
      "log : 140\n",
      "log : 150\n",
      "***epoch*** :  8\n",
      "-- train 0.67207(69.8%), valid0.70769(68.5%)\n",
      "-- train 0.69510(69.0%), valid0.71680(68.6%)\n",
      "-- train 0.69364(69.1%), valid0.71907(68.5%)\n",
      "-- train 0.71065(68.6%), valid0.72857(68.1%)\n",
      "-- train 0.76791(66.2%), valid0.78602(65.9%)\n",
      "-- train 0.73685(67.2%), valid0.75495(66.6%)\n",
      "-- train 0.66519(70.4%), valid0.70274(69.1%)\n",
      "-- train 0.69232(69.5%), valid0.71623(68.4%)\n",
      "-- train 0.68251(69.5%), valid0.71710(68.7%)\n",
      "-- train 0.69717(68.8%), valid0.72129(67.8%)\n",
      "-- train 0.73311(67.6%), valid0.75244(66.8%)\n",
      "-- train 0.75107(66.4%), valid0.76798(66.0%)\n",
      "-- train 0.66338(70.3%), valid0.70188(68.9%)\n",
      "-- train 0.69754(68.9%), valid0.71836(68.4%)\n",
      "-- train 0.67360(69.8%), valid0.71125(68.5%)\n",
      "-- train 0.68857(69.2%), valid0.70870(68.6%)\n",
      "-- train 0.73514(67.7%), valid0.74877(67.3%)\n",
      "-- train 0.73744(66.7%), valid0.75474(66.5%)\n",
      "Accuracy: 0.685499995947\n",
      "Accuracy: 0.685949996114\n",
      "Accuracy: 0.684849996865\n",
      "Accuracy: 0.680949996412\n",
      "Accuracy: 0.658600002527\n",
      "Accuracy: 0.6659000054\n",
      "Accuracy: 0.691099998355\n",
      "Accuracy: 0.684450000525\n",
      "Accuracy: 0.687349998951\n",
      "Accuracy: 0.677949993312\n",
      "Accuracy: 0.66835000217\n",
      "Accuracy: 0.660249997675\n",
      "Accuracy: 0.689050000906\n",
      "Accuracy: 0.684449997544\n",
      "Accuracy: 0.685149995983\n",
      "Accuracy: 0.685750000179\n",
      "Accuracy: 0.672699996829\n",
      "Accuracy: 0.664500007033\n",
      " \n",
      "log : 0\n",
      "log : 10\n",
      "log : 20\n",
      "log : 30\n",
      "log : 40\n",
      "log : 50\n",
      "log : 60\n",
      "log : 70\n",
      "log : 80\n",
      "log : 90\n",
      "log : 100\n",
      "log : 110\n",
      "log : 120\n",
      "log : 130\n",
      "log : 140\n",
      "log : 150\n",
      "***epoch*** :  9\n",
      "-- train 0.66885(70.1%), valid0.70892(68.7%)\n",
      "-- train 0.67890(69.9%), valid0.70718(69.1%)\n",
      "-- train 0.68620(69.5%), valid0.71125(68.7%)\n",
      "-- train 0.71595(68.2%), valid0.73576(67.7%)\n",
      "-- train 0.78947(64.5%), valid0.80287(64.2%)\n",
      "-- train 0.74496(66.6%), valid0.76325(66.1%)\n",
      "-- train 0.66702(70.5%), valid0.71058(69.1%)\n",
      "-- train 0.69071(68.8%), valid0.71562(67.8%)\n",
      "-- train 0.67903(70.1%), valid0.71790(68.9%)\n",
      "-- train 0.70267(68.1%), valid0.72760(67.0%)\n",
      "-- train 0.72013(68.1%), valid0.73975(67.9%)\n",
      "-- train 0.73452(67.8%), valid0.75553(67.1%)\n",
      "-- train 0.66918(70.1%), valid0.71053(68.6%)\n",
      "-- train 0.69146(69.4%), valid0.71565(68.7%)\n",
      "-- train 0.66263(70.3%), valid0.70668(69.1%)\n",
      "-- train 0.69522(69.0%), valid0.71471(68.3%)\n",
      "-- train 0.72807(67.2%), valid0.74923(66.7%)\n",
      "-- train 0.73509(67.8%), valid0.75530(67.3%)\n",
      "Accuracy: 0.687000006437\n",
      "Accuracy: 0.690500003099\n",
      "Accuracy: 0.687250003219\n",
      "Accuracy: 0.676949998736\n",
      "Accuracy: 0.641799995303\n",
      "Accuracy: 0.661149999499\n",
      "Accuracy: 0.690749999881\n",
      "Accuracy: 0.678049997985\n",
      "Accuracy: 0.688999995589\n",
      "Accuracy: 0.669650000334\n",
      "Accuracy: 0.678650000691\n",
      "Accuracy: 0.67064999938\n",
      "Accuracy: 0.686399994791\n",
      "Accuracy: 0.687250004709\n",
      "Accuracy: 0.690699997544\n",
      "Accuracy: 0.683349995315\n",
      "Accuracy: 0.666599997878\n",
      "Accuracy: 0.672899998724\n",
      " \n",
      "log : 0\n",
      "log : 10\n",
      "log : 20\n",
      "log : 30\n",
      "log : 40\n",
      "log : 50\n",
      "log : 60\n",
      "log : 70\n",
      "log : 80\n",
      "log : 90\n",
      "log : 100\n",
      "log : 110\n",
      "log : 120\n",
      "log : 130\n",
      "log : 140\n",
      "log : 150\n",
      "***epoch*** :  10\n",
      "-- train 0.62973(71.5%), valid0.69324(69.7%)\n",
      "-- train 0.66011(70.6%), valid0.70050(69.5%)\n",
      "-- train 0.66153(70.4%), valid0.70596(69.3%)\n",
      "-- train 0.68136(69.6%), valid0.70990(68.8%)\n",
      "-- train 0.74852(67.4%), valid0.76863(66.8%)\n",
      "-- train 0.71356(68.1%), valid0.74206(67.2%)\n",
      "-- train 0.62665(71.8%), valid0.70257(69.8%)\n",
      "-- train 0.65625(70.8%), valid0.69627(69.4%)\n",
      "-- train 0.64083(71.0%), valid0.70125(69.6%)\n",
      "-- train 0.65784(70.5%), valid0.69455(69.2%)\n",
      "-- train 0.69588(69.0%), valid0.72587(68.4%)\n",
      "-- train 0.70603(68.5%), valid0.74176(67.4%)\n",
      "-- train 0.62189(72.0%), valid0.68869(69.5%)\n",
      "-- train 0.66403(70.2%), valid0.69740(69.3%)\n",
      "-- train 0.63104(71.4%), valid0.70026(69.5%)\n",
      "-- train 0.67004(69.9%), valid0.70004(69.2%)\n",
      "-- train 0.69668(68.3%), valid0.72924(67.3%)\n",
      "-- train 0.69604(68.7%), valid0.72426(68.0%)\n",
      "Accuracy: 0.696749997139\n",
      "Accuracy: 0.69469999969\n",
      "Accuracy: 0.692699997127\n",
      "Accuracy: 0.687549997866\n",
      "Accuracy: 0.667549997568\n",
      "Accuracy: 0.672049997747\n",
      "Accuracy: 0.698199996352\n",
      "Accuracy: 0.694249999523\n",
      "Accuracy: 0.695849998295\n",
      "Accuracy: 0.692399998009\n",
      "Accuracy: 0.684399998188\n",
      "Accuracy: 0.673749999702\n",
      "Accuracy: 0.695299997926\n",
      "Accuracy: 0.692599999905\n",
      "Accuracy: 0.695150004327\n",
      "Accuracy: 0.692050002515\n",
      "Accuracy: 0.673349994421\n",
      "Accuracy: 0.680350001156\n",
      " \n",
      "log : 0\n",
      "log : 10\n",
      "log : 20\n",
      "log : 30\n",
      "log : 40\n",
      "log : 50\n",
      "log : 60\n",
      "log : 70\n",
      "log : 80\n",
      "log : 90\n",
      "log : 100\n",
      "log : 110\n",
      "log : 120\n",
      "log : 130\n",
      "log : 140\n",
      "log : 150\n",
      "***epoch*** :  11\n",
      "-- train 0.61415(72.3%), valid0.69326(69.8%)\n",
      "-- train 0.64915(70.8%), valid0.69681(69.6%)\n",
      "-- train 0.64901(70.8%), valid0.70208(69.7%)\n",
      "-- train 0.67460(70.0%), valid0.70926(68.9%)\n",
      "-- train 0.73740(67.5%), valid0.76129(67.1%)\n",
      "-- train 0.70143(68.4%), valid0.73750(67.6%)\n",
      "-- train 0.61672(72.2%), valid0.71402(69.4%)\n",
      "-- train 0.65046(71.1%), valid0.69720(69.6%)\n",
      "-- train 0.62875(71.4%), valid0.69988(69.7%)\n",
      "-- train 0.65173(70.9%), valid0.69584(69.3%)\n",
      "-- train 0.68370(69.4%), valid0.71968(68.5%)\n",
      "-- train 0.69798(69.1%), valid0.73651(67.9%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- train 0.61158(72.3%), valid0.69739(69.5%)\n",
      "-- train 0.65521(70.8%), valid0.69791(69.5%)\n",
      "-- train 0.61919(72.0%), valid0.70804(69.4%)\n",
      "-- train 0.65649(70.4%), valid0.69085(69.7%)\n",
      "-- train 0.67625(69.5%), valid0.71608(68.5%)\n",
      "-- train 0.68943(69.3%), valid0.72294(68.4%)\n",
      "Accuracy: 0.69810000211\n",
      "Accuracy: 0.696150003374\n",
      "Accuracy: 0.697299996018\n",
      "Accuracy: 0.688750000298\n",
      "Accuracy: 0.671250000596\n",
      "Accuracy: 0.676199999452\n",
      "Accuracy: 0.694399999082\n",
      "Accuracy: 0.695950001478\n",
      "Accuracy: 0.697199995816\n",
      "Accuracy: 0.692650000751\n",
      "Accuracy: 0.68504999727\n",
      "Accuracy: 0.678700000048\n",
      "Accuracy: 0.695349995792\n",
      "Accuracy: 0.694649998844\n",
      "Accuracy: 0.694449998438\n",
      "Accuracy: 0.696750000119\n",
      "Accuracy: 0.684949998558\n",
      "Accuracy: 0.683749999106\n",
      " \n",
      "log : 0\n",
      "log : 10\n",
      "log : 20\n",
      "log : 30\n",
      "log : 40\n",
      "log : 50\n",
      "log : 60\n",
      "log : 70\n",
      "log : 80\n",
      "log : 90\n",
      "log : 100\n",
      "log : 110\n",
      "log : 120\n",
      "log : 130\n",
      "log : 140\n",
      "log : 150\n",
      "***epoch*** :  12\n",
      "-- train 0.61064(72.4%), valid0.70302(69.6%)\n",
      "-- train 0.64185(71.2%), valid0.69345(69.8%)\n",
      "-- train 0.64622(71.0%), valid0.70767(69.5%)\n",
      "-- train 0.66607(70.4%), valid0.70421(69.5%)\n",
      "-- train 0.73369(67.6%), valid0.76159(67.0%)\n",
      "-- train 0.69636(68.4%), valid0.73828(67.2%)\n",
      "-- train 0.61390(72.2%), valid0.72508(69.2%)\n",
      "-- train 0.63750(71.6%), valid0.69426(69.6%)\n",
      "-- train 0.63728(71.1%), valid0.71622(69.2%)\n",
      "-- train 0.64311(71.1%), valid0.69403(69.3%)\n",
      "-- train 0.67493(69.9%), valid0.71821(68.8%)\n",
      "-- train 0.68674(69.4%), valid0.73207(67.9%)\n",
      "-- train 0.60067(72.8%), valid0.70627(69.5%)\n",
      "-- train 0.64478(70.9%), valid0.69664(69.6%)\n",
      "-- train 0.60958(72.2%), valid0.71111(69.5%)\n",
      "-- train 0.64812(70.9%), valid0.69054(69.5%)\n",
      "-- train 0.66691(70.0%), valid0.71273(68.8%)\n",
      "-- train 0.68001(69.6%), valid0.71802(68.5%)\n",
      "Accuracy: 0.696249999106\n",
      "Accuracy: 0.698149999976\n",
      "Accuracy: 0.694599997997\n",
      "Accuracy: 0.69469999671\n",
      "Accuracy: 0.670199994743\n",
      "Accuracy: 0.671950002015\n",
      "Accuracy: 0.692149996758\n",
      "Accuracy: 0.696449998021\n",
      "Accuracy: 0.691749997437\n",
      "Accuracy: 0.692599999905\n",
      "Accuracy: 0.687549999356\n",
      "Accuracy: 0.679399995506\n",
      "Accuracy: 0.695000000298\n",
      "Accuracy: 0.695600001514\n",
      "Accuracy: 0.694500000775\n",
      "Accuracy: 0.694899997115\n",
      "Accuracy: 0.687549999356\n",
      "Accuracy: 0.685350000858\n",
      " \n",
      "log : 0\n",
      "log : 10\n",
      "log : 20\n",
      "log : 30\n",
      "log : 40\n",
      "log : 50\n",
      "log : 60\n",
      "log : 70\n",
      "log : 80\n",
      "log : 90\n",
      "log : 100\n",
      "log : 110\n",
      "log : 120\n",
      "log : 130\n",
      "log : 140\n",
      "log : 150\n",
      "***epoch*** :  13\n",
      "-- train 0.60038(72.8%), valid0.70557(69.8%)\n",
      "-- train 0.63434(71.6%), valid0.69744(69.5%)\n",
      "-- train 0.63303(71.7%), valid0.70638(69.8%)\n",
      "-- train 0.66187(70.4%), valid0.70609(69.2%)\n",
      "-- train 0.72297(67.3%), valid0.75637(66.4%)\n",
      "-- train 0.69445(69.2%), valid0.73889(68.0%)\n",
      "-- train 0.60930(72.4%), valid0.72493(69.3%)\n",
      "-- train 0.62997(71.9%), valid0.69295(69.7%)\n",
      "-- train 0.62243(71.8%), valid0.71620(69.2%)\n",
      "-- train 0.63749(71.4%), valid0.69261(69.4%)\n",
      "-- train 0.67508(69.6%), valid0.72019(68.6%)\n",
      "-- train 0.68072(69.8%), valid0.73026(68.3%)\n",
      "-- train 0.60156(72.7%), valid0.71530(69.3%)\n",
      "-- train 0.63887(71.2%), valid0.69286(69.5%)\n",
      "-- train 0.62041(72.1%), valid0.73183(69.1%)\n",
      "-- train 0.64422(71.2%), valid0.69146(69.7%)\n",
      "-- train 0.66044(70.2%), valid0.71161(68.9%)\n",
      "-- train 0.67665(69.7%), valid0.72114(68.5%)\n",
      "Accuracy: 0.69755000025\n",
      "Accuracy: 0.694849996269\n",
      "Accuracy: 0.697799994051\n",
      "Accuracy: 0.691900001466\n",
      "Accuracy: 0.663699999452\n",
      "Accuracy: 0.67974999994\n",
      "Accuracy: 0.693149997294\n",
      "Accuracy: 0.697399999201\n",
      "Accuracy: 0.692449997365\n",
      "Accuracy: 0.694199995697\n",
      "Accuracy: 0.68599999845\n",
      "Accuracy: 0.682899998128\n",
      "Accuracy: 0.693349999189\n",
      "Accuracy: 0.694749997556\n",
      "Accuracy: 0.690899996459\n",
      "Accuracy: 0.697349998355\n",
      "Accuracy: 0.688600000739\n",
      "Accuracy: 0.68464999944\n",
      " \n",
      "log : 0\n",
      "log : 10\n",
      "log : 20\n",
      "log : 30\n",
      "log : 40\n",
      "log : 50\n",
      "log : 60\n",
      "log : 70\n",
      "log : 80\n",
      "log : 90\n",
      "log : 100\n",
      "log : 110\n",
      "log : 120\n",
      "log : 130\n",
      "log : 140\n",
      "log : 150\n",
      "***epoch*** :  14\n",
      "-- train 0.60747(72.7%), valid0.71669(69.4%)\n",
      "-- train 0.62873(71.8%), valid0.70070(69.9%)\n",
      "-- train 0.62249(71.9%), valid0.70360(69.8%)\n",
      "-- train 0.65634(70.5%), valid0.70294(69.2%)\n",
      "-- train 0.71928(67.4%), valid0.75684(66.5%)\n",
      "-- train 0.68976(69.2%), valid0.74126(67.7%)\n",
      "-- train 0.59781(73.2%), valid0.73251(69.4%)\n",
      "-- train 0.62750(72.0%), valid0.69839(69.6%)\n",
      "-- train 0.61605(72.2%), valid0.71837(69.4%)\n",
      "-- train 0.63110(71.8%), valid0.69632(69.6%)\n",
      "-- train 0.66393(70.2%), valid0.71217(68.9%)\n",
      "-- train 0.67128(70.0%), valid0.73199(67.8%)\n",
      "-- train 0.60004(72.7%), valid0.72663(69.4%)\n",
      "-- train 0.64126(71.1%), valid0.69369(69.6%)\n",
      "-- train 0.60493(72.4%), valid0.72535(69.2%)\n",
      "-- train 0.63878(71.3%), valid0.69409(69.9%)\n",
      "-- train 0.65419(70.3%), valid0.70904(68.9%)\n",
      "-- train 0.68019(69.5%), valid0.72724(68.0%)\n",
      "Accuracy: 0.694350001216\n",
      "Accuracy: 0.698600000143\n",
      "Accuracy: 0.697500000894\n",
      "Accuracy: 0.691500003636\n",
      "Accuracy: 0.664750000834\n",
      "Accuracy: 0.676699997485\n",
      "Accuracy: 0.694249998033\n",
      "Accuracy: 0.696000002325\n",
      "Accuracy: 0.693799994886\n",
      "Accuracy: 0.695749996603\n",
      "Accuracy: 0.688599997759\n",
      "Accuracy: 0.67840000093\n",
      "Accuracy: 0.693999998271\n",
      "Accuracy: 0.696350003779\n",
      "Accuracy: 0.692149995267\n",
      "Accuracy: 0.699199996889\n",
      "Accuracy: 0.689149996638\n",
      "Accuracy: 0.679999998212\n",
      " \n",
      "log : 0\n",
      "log : 10\n",
      "log : 20\n",
      "log : 30\n",
      "log : 40\n",
      "log : 50\n",
      "log : 60\n",
      "log : 70\n",
      "log : 80\n",
      "log : 90\n",
      "log : 100\n",
      "log : 110\n",
      "log : 120\n",
      "log : 130\n",
      "log : 140\n",
      "log : 150\n",
      "***epoch*** :  15\n",
      "-- train 0.60374(72.7%), valid0.72203(69.3%)\n",
      "-- train 0.62733(71.8%), valid0.70131(69.9%)\n",
      "-- train 0.61876(72.1%), valid0.70957(69.6%)\n",
      "-- train 0.64905(70.9%), valid0.70057(69.4%)\n",
      "-- train 0.71578(67.7%), valid0.75481(66.7%)\n",
      "-- train 0.68812(68.9%), valid0.74314(67.6%)\n",
      "-- train 0.59343(73.2%), valid0.74423(69.2%)\n",
      "-- train 0.62193(72.1%), valid0.69743(69.6%)\n",
      "-- train 0.61561(72.2%), valid0.72812(68.9%)\n",
      "-- train 0.62979(71.7%), valid0.69420(69.5%)\n",
      "-- train 0.66469(70.0%), valid0.71501(68.6%)\n",
      "-- train 0.66502(70.1%), valid0.73319(68.2%)\n",
      "-- train 0.58965(73.2%), valid0.72665(69.4%)\n",
      "-- train 0.63024(71.7%), valid0.69466(69.5%)\n",
      "-- train 0.59397(73.1%), valid0.72217(69.5%)\n",
      "-- train 0.63634(71.4%), valid0.69501(69.6%)\n",
      "-- train 0.64763(70.7%), valid0.70828(68.9%)\n",
      "-- train 0.66845(69.9%), valid0.71749(68.5%)\n",
      "Accuracy: 0.69319999814\n",
      "Accuracy: 0.698849992454\n",
      "Accuracy: 0.695949998498\n",
      "Accuracy: 0.693949994445\n",
      "Accuracy: 0.667349995673\n",
      "Accuracy: 0.675699995458\n",
      "Accuracy: 0.691599999368\n",
      "Accuracy: 0.695949995518\n",
      "Accuracy: 0.689499995112\n",
      "Accuracy: 0.694849999249\n",
      "Accuracy: 0.686399996281\n",
      "Accuracy: 0.681850004196\n",
      "Accuracy: 0.694399997592\n",
      "Accuracy: 0.695100000501\n",
      "Accuracy: 0.695199996233\n",
      "Accuracy: 0.696299999952\n",
      "Accuracy: 0.689399990439\n",
      "Accuracy: 0.684999996424\n",
      " \n",
      "log : 0\n",
      "log : 10\n",
      "log : 20\n",
      "log : 30\n",
      "log : 40\n",
      "log : 50\n",
      "log : 60\n",
      "log : 70\n",
      "log : 80\n",
      "log : 90\n",
      "log : 100\n",
      "log : 110\n",
      "log : 120\n",
      "log : 130\n",
      "log : 140\n",
      "log : 150\n",
      "***epoch*** :  16\n",
      "-- train 0.59241(73.5%), valid0.72550(69.7%)\n",
      "-- train 0.62198(72.2%), valid0.69583(70.0%)\n",
      "-- train 0.61807(72.2%), valid0.71742(69.8%)\n",
      "-- train 0.65252(70.7%), valid0.71030(69.0%)\n",
      "-- train 0.71913(67.3%), valid0.76061(66.2%)\n",
      "-- train 0.68040(69.1%), valid0.73957(67.6%)\n",
      "-- train 0.59228(72.9%), valid0.75299(69.1%)\n",
      "-- train 0.62096(72.4%), valid0.69679(69.9%)\n",
      "-- train 0.61654(72.1%), valid0.72901(69.4%)\n",
      "-- train 0.63420(71.4%), valid0.70134(69.1%)\n",
      "-- train 0.65452(70.2%), valid0.71499(68.6%)\n",
      "-- train 0.65461(70.5%), valid0.73159(68.2%)\n",
      "-- train 0.57734(73.6%), valid0.72787(69.3%)\n",
      "-- train 0.62663(71.9%), valid0.69421(69.8%)\n",
      "-- train 0.59964(72.7%), valid0.72462(69.5%)\n",
      "-- train 0.62809(71.5%), valid0.69923(69.6%)\n",
      "-- train 0.64530(70.7%), valid0.70693(69.1%)\n",
      "-- train 0.67279(69.8%), valid0.72751(68.0%)\n",
      "Accuracy: 0.697049999237\n",
      "Accuracy: 0.700349995494\n",
      "Accuracy: 0.698150002956\n",
      "Accuracy: 0.689800000191\n",
      "Accuracy: 0.662350000441\n",
      "Accuracy: 0.676049998403\n",
      "Accuracy: 0.690949995816\n",
      "Accuracy: 0.698649996519\n",
      "Accuracy: 0.693649999797\n",
      "Accuracy: 0.690500001609\n",
      "Accuracy: 0.685649995506\n",
      "Accuracy: 0.6821999982\n",
      "Accuracy: 0.693249998987\n",
      "Accuracy: 0.698250000179\n",
      "Accuracy: 0.694550000131\n",
      "Accuracy: 0.695549997687\n",
      "Accuracy: 0.690949998796\n",
      "Accuracy: 0.679500000179\n",
      " \n",
      "log : 0\n",
      "log : 10\n",
      "log : 20\n",
      "log : 30\n",
      "log : 40\n",
      "log : 50\n",
      "log : 60\n",
      "log : 70\n",
      "log : 80\n",
      "log : 90\n",
      "log : 100\n",
      "log : 110\n",
      "log : 120\n",
      "log : 130\n",
      "log : 140\n",
      "log : 150\n",
      "***epoch*** :  17\n",
      "-- train 0.58795(73.5%), valid0.73447(69.7%)\n",
      "-- train 0.61671(72.4%), valid0.70651(69.7%)\n",
      "-- train 0.61599(72.5%), valid0.71982(69.5%)\n",
      "-- train 0.64395(71.0%), valid0.70441(69.3%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- train 0.71664(67.2%), valid0.76194(66.2%)\n",
      "-- train 0.67788(68.8%), valid0.74578(67.1%)\n",
      "-- train 0.59104(73.3%), valid0.75339(69.1%)\n",
      "-- train 0.62370(72.1%), valid0.70271(69.6%)\n",
      "-- train 0.60852(72.4%), valid0.73377(69.1%)\n",
      "-- train 0.62508(71.9%), valid0.70059(69.1%)\n",
      "-- train 0.65238(70.4%), valid0.71643(68.7%)\n",
      "-- train 0.65250(70.7%), valid0.73684(68.2%)\n",
      "-- train 0.56559(74.3%), valid0.73516(69.7%)\n",
      "-- train 0.62560(71.7%), valid0.69557(69.6%)\n",
      "-- train 0.59446(73.2%), valid0.73618(69.4%)\n",
      "-- train 0.63436(71.6%), valid0.70532(69.7%)\n",
      "-- train 0.64616(70.7%), valid0.71315(68.9%)\n",
      "-- train 0.65715(70.4%), valid0.71462(68.7%)\n",
      "Accuracy: 0.696950000525\n",
      "Accuracy: 0.696750000119\n",
      "Accuracy: 0.694799993932\n",
      "Accuracy: 0.692550000548\n",
      "Accuracy: 0.661900000274\n",
      "Accuracy: 0.671400000155\n",
      "Accuracy: 0.690999998152\n",
      "Accuracy: 0.696499998868\n",
      "Accuracy: 0.690650004148\n",
      "Accuracy: 0.691199998558\n",
      "Accuracy: 0.686999996006\n",
      "Accuracy: 0.682049995661\n",
      "Accuracy: 0.696900002658\n",
      "Accuracy: 0.6956499964\n",
      "Accuracy: 0.693999999762\n",
      "Accuracy: 0.696549995244\n",
      "Accuracy: 0.688899996877\n",
      "Accuracy: 0.687349997461\n",
      " \n",
      "log : 0\n",
      "log : 10\n",
      "log : 20\n",
      "log : 30\n",
      "log : 40\n",
      "log : 50\n",
      "log : 60\n",
      "log : 70\n",
      "log : 80\n",
      "log : 90\n",
      "log : 100\n",
      "log : 110\n",
      "log : 120\n",
      "log : 130\n",
      "log : 140\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-db6de48a297d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodel_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_lst\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_xs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0mavg_cost\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_num\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtotal_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-8a8f6769ce29>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X_input, Y_input, learning_rate, training)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mY_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('Learning Started!')\n",
    "print(\"\")\n",
    "\n",
    "# train my model\n",
    "for epoch in range(total_epoch):\n",
    "    avg_cost = [0]*len(model_lst)\n",
    "    total_batch = int(len(training_lst) / batch_size)\n",
    "    idx = 0\n",
    "    \n",
    "    if epoch == 0 :\n",
    "        learning_rate = learning_rate1\n",
    "    elif epoch == 10 :\n",
    "        learning_rate = learning_rate2\n",
    "    elif epoch == 20 :\n",
    "        learning_rate = learning_rate3\n",
    "    elif epoch == 80 :\n",
    "        learning_rate = learning_rate4\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = training_lst[idx:idx+batch_size],training_label[idx:idx+batch_size]\n",
    "        \n",
    "        for model_num, model in enumerate(model_lst) :\n",
    "            _, c = model.train(batch_xs, batch_ys, learning_rate)\n",
    "            avg_cost[model_num] += c / total_batch\n",
    "        \n",
    "        idx += batch_size\n",
    "        if i%10 == 0 :\n",
    "            print(\"log :\", i)\n",
    "            \n",
    "    #train/valid cost & acc\n",
    "    print(\"***epoch*** : \", epoch)\n",
    "    for model_num, model in enumerate(model_lst) :\n",
    "        train_cost, train_acc = model.evaluate(training_lst, training_label)\n",
    "        valid_cost, valid_acc = model.evaluate(valid_lst, valid_label)\n",
    "\n",
    "        tl_ta_vl_va_lst[model_num][0].append(train_cost)\n",
    "        tl_ta_vl_va_lst[model_num][1].append(train_acc)\n",
    "        tl_ta_vl_va_lst[model_num][2].append(valid_cost)\n",
    "        tl_ta_vl_va_lst[model_num][3].append(valid_acc)\n",
    "\n",
    "        print(\"-- train {:.5f}({:.1f}%), valid{:.5f}({:.1f}%)\".format(train_cost, train_acc*100, valid_cost, valid_acc*100))\n",
    "    \n",
    "    for model in model_lst :\n",
    "        print('Accuracy:', model.evaluate(valid_lst, valid_label)[1])\n",
    "    print(\" \")\n",
    "\n",
    "print(\"\")\n",
    "print('Learning Finished!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <br></br><br></br><br></br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for idx in range(len(model_lst)) :\n",
    "    plt.plot(tl_ta_vl_va_lst[idx][0], label='training'+str(idx))\n",
    "    plt.plot(tl_ta_vl_va_lst[idx][2], label='valid'+str(idx))\n",
    "    plt.title(\"model\"+str(idx))\n",
    "    plt.grid(\"on\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for idx in range(len(model_lst)) :\n",
    "    plt.plot(tl_ta_vl_va_lst[idx][1], label='training'+str(idx))\n",
    "    plt.plot(tl_ta_vl_va_lst[idx][3], label='valid'+str(idx))\n",
    "    plt.title(\"model\"+str(idx))\n",
    "    plt.grid(\"on\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for idx in range(len(model_lst)) :\n",
    "    plt.plot(tl_ta_vl_va_lst[idx][0], label='training'+str(idx))\n",
    "    \n",
    "plt.grid(\"on\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for idx in range(len(model_lst)) :\n",
    "    plt.plot(tl_ta_vl_va_lst[idx][2], label='training'+str(idx))\n",
    "    \n",
    "plt.grid(\"on\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for idx in range(len(model_lst)) :\n",
    "    plt.plot(tl_ta_vl_va_lst[idx][1], label='training'+str(idx))\n",
    "    \n",
    "plt.grid(\"on\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for idx in range(len(model_lst)) :\n",
    "    plt.plot(tl_ta_vl_va_lst[idx][3], label='training'+str(idx))\n",
    "    \n",
    "plt.grid(\"on\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <br></br><br></br><br></br>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tf.reset_default_graph() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./advanced_RNN/original_user_vector/original'"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "for idx, model in enumerate(model_lst) :\n",
    "    saver.save(model.sess, './model/MLP_default_model_{}'.format(idx))\n",
    "\n",
    "print(\"Saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br><br></br><br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def making_data(csv) :\n",
    "    activity = pd.read_csv(csv).drop(\"Unnamed: 0\", axis=1)\n",
    "    \n",
    "    activity = activity[activity[\"label\"] == \"empty\"]\n",
    "    activity = activity.drop(\"label\", axis=1)\n",
    "    \n",
    "    activity = activity.sort_values([\"acc_id\",\"wk\"])[['acc_id', 'wk', 'cnt_clear_bam', 'cnt_clear_inzone_light','cnt_clear_inzone_normal', \n",
    "                                                  'cnt_clear_inzone_skilled', 'cnt_clear_inzone_solo', 'cnt_clear_raid', 'cnt_clear_raid_light',\n",
    "                                                  'cnt_dt', 'cnt_enter_bam', 'cnt_enter_inzone_light', 'cnt_enter_inzone_normal', \n",
    "                                                  'cnt_enter_inzone_skilled', 'cnt_enter_inzone_solo', 'cnt_enter_raid', 'cnt_enter_raid_light',\n",
    "                                                  'cnt_use_buffitem', 'district_chat', 'duel_cnt', 'duel_win', 'faction_chat', 'game_combat_time', \n",
    "                                                  'gathering_cnt', 'get_money','guild_chat', 'item_hongmun', 'making_cnt', 'normal_chat', \n",
    "                                                  'npc_exp', 'npc_hongmun', 'party_chat', 'partybattle_cnt', 'partybattle_win', 'play_time', \n",
    "                                                  'quest_exp', 'quest_hongmun', 'whisper_chat','first_week', 'payment_amount']]\n",
    "    \n",
    "    activity1 = activity[activity[\"wk\"]==1].drop([\"acc_id\", \"wk\"], axis=1)\n",
    "    activity2 = activity[activity[\"wk\"]==2].drop([\"acc_id\", \"wk\"], axis=1)\n",
    "    activity3 = activity[activity[\"wk\"]==3].drop([\"acc_id\", \"wk\"], axis=1)\n",
    "    activity4 = activity[activity[\"wk\"]==4].drop([\"acc_id\", \"wk\"], axis=1)\n",
    "    activity5 = activity[activity[\"wk\"]==5].drop([\"acc_id\", \"wk\"], axis=1)\n",
    "    activity6 = activity[activity[\"wk\"]==6].drop([\"acc_id\", \"wk\"], axis=1)\n",
    "    activity7 = activity[activity[\"wk\"]==7].drop([\"acc_id\", \"wk\"], axis=1)\n",
    "    activity8 = activity[activity[\"wk\"]==8].drop([\"acc_id\", \"wk\"], axis=1)\n",
    "    \n",
    "    label = activity[\"acc_id\"].values\n",
    "    activity = np.concatenate([activity1.values, activity2.values, activity3.values, activity4.values,\n",
    "                               activity5.values, activity6.values, activity7.values, activity8.values], axis=1)\n",
    "\n",
    "    total_lst = activity\n",
    "    return total_lst, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data, test_acc_id = making_data(\"OnlyExpanded.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br><br></br><br></br> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = []\n",
    "for model in model_lst :\n",
    "    result.append(np.argmax(model.predict(test_data), axis=2)) \n",
    "    \n",
    "result = list(map(lambda x : x.tolist()[0], result))\n",
    "for r_lst in result :\n",
    "    print(\"week: {}, month: {}, 2month: {}, retained: {}\".format(r_lst.count(0), r_lst.count(1), r_lst.count(2), r_lst.count(3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_df = pd.DataFrame(sorted(list(set(list(test_acc_id))))).rename(columns = {0 : \"acc_id\"})\n",
    "result_df = pd.DataFrame(result).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result_df2 = pd.concat([label_df, result_df], axis=1)\n",
    "result_df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
