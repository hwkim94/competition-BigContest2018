{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(lst, num_class=4) :\n",
    "    return np.eye(num_class)[lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def making_data(csv) :\n",
    "    activity = pd.read_csv(csv).drop(\"Unnamed: 0\", axis=1)\n",
    "    label = activity[[\"acc_id\", \"label\"]]\n",
    "    \n",
    "    activity = activity[activity[\"label\"] != \"empty\"]\n",
    "    activity = activity.drop(\"label\", axis=1)\n",
    "    label = label[label[\"label\"] != \"empty\"]\n",
    "    \n",
    "    activity = activity.sort_values([\"acc_id\",\"wk\"])[['acc_id', 'wk', 'cnt_clear_bam', 'cnt_clear_inzone_light','cnt_clear_inzone_normal', \n",
    "                                                  'cnt_clear_inzone_skilled', 'cnt_clear_inzone_solo', 'cnt_clear_raid', 'cnt_clear_raid_light',\n",
    "                                                  'cnt_dt', 'cnt_enter_bam', 'cnt_enter_inzone_light', 'cnt_enter_inzone_normal', \n",
    "                                                  'cnt_enter_inzone_skilled', 'cnt_enter_inzone_solo', 'cnt_enter_raid', 'cnt_enter_raid_light',\n",
    "                                                  'cnt_use_buffitem', 'district_chat', 'duel_cnt', 'duel_win', 'faction_chat', 'game_combat_time', \n",
    "                                                  'gathering_cnt', 'get_money','guild_chat', 'item_hongmun', 'making_cnt', 'normal_chat', \n",
    "                                                  'npc_exp', 'npc_hongmun', 'party_chat', 'partybattle_cnt', 'partybattle_win', 'play_time', \n",
    "                                                  'quest_exp', 'quest_hongmun', 'whisper_chat','first_week', 'payment_amount']]\n",
    "    label = label.sort_values(\"acc_id\")\n",
    "    \n",
    "    label_lst = sorted(list(set([tuple(x) for x in label.values])))\n",
    "    label = pd.DataFrame(label_lst, columns = [\"acc_id\", \"label\"])\n",
    "    \n",
    "    activity1 = activity[activity[\"wk\"]==1].drop([\"acc_id\", \"wk\"], axis=1)\n",
    "    activity2 = activity[activity[\"wk\"]==2].drop([\"acc_id\", \"wk\"], axis=1)\n",
    "    activity3 = activity[activity[\"wk\"]==3].drop([\"acc_id\", \"wk\"], axis=1)\n",
    "    activity4 = activity[activity[\"wk\"]==4].drop([\"acc_id\", \"wk\"], axis=1)\n",
    "    activity5 = activity[activity[\"wk\"]==5].drop([\"acc_id\", \"wk\"], axis=1)\n",
    "    activity6 = activity[activity[\"wk\"]==6].drop([\"acc_id\", \"wk\"], axis=1)\n",
    "    activity7 = activity[activity[\"wk\"]==7].drop([\"acc_id\", \"wk\"], axis=1)\n",
    "    activity8 = activity[activity[\"wk\"]==8].drop([\"acc_id\", \"wk\"], axis=1)\n",
    "    \n",
    "    activity = np.concatenate([activity1.values, activity2.values, activity3.values, activity4.values,\n",
    "                               activity5.values, activity6.values, activity7.values, activity8.values], axis=1)\n",
    "    \n",
    "    label_dic = {\"week\":0 , \"month\" :1, \"2month\":2, \"retained\":3}\n",
    "\n",
    "    label2 = label.sort_values(by=\"acc_id\")\n",
    "    label2[\"label\"] = label2[\"label\"].map(lambda x : label_dic[x])\n",
    "    \n",
    "    total_lst = activity\n",
    "    label_dic = label2.label.tolist()\n",
    "    total_label = one_hot(label_dic)\n",
    "    \n",
    "    return total_lst, total_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br><br></br><br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_lst, total_label = making_data(\"OnlyExpanded.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx1 = len(total_lst)//5 *4\n",
    "\n",
    "training_lst = np.array(total_lst[:idx1])\n",
    "valid_lst = np.array(total_lst[idx1:])\n",
    "\n",
    "training_label = np.array(total_label[:idx1])\n",
    "valid_label = np.array(total_label[idx1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.array(total_lst).shape)\n",
    "print(np.array(total_label).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br><br></br><br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP() :\n",
    "    def __init__(self, sess, name):\n",
    "        self.sess = sess\n",
    "        self.name = name\n",
    "        \n",
    "    \n",
    "    def build(self, batch_size, input_dim, output_dim, num_layer, num_unit, activation) :\n",
    "        with tf.variable_scope(self.name) :\n",
    "            \n",
    "            ## Setting ##\n",
    "            # input  : ? x input_length x input_dim\n",
    "            self.X = tf.placeholder(tf.float32, [None, input_dim])\n",
    "            self.Y = tf.placeholder(tf.float32, [None, output_dim])\n",
    "            self.learning_rate =  tf.placeholder(tf.float32)\n",
    "            self.training = tf.placeholder(tf.bool)\n",
    "            \n",
    "            self.batch_size = batch_size\n",
    "            self.output_dim = output_dim\n",
    "            self.num_layer = num_layer\n",
    "            self.num_unit = num_unit\n",
    "            self.activation = activation\n",
    "            #############\n",
    "            \n",
    "            \n",
    "            ## MLP ##\n",
    "            layer = tf.layers.dense(self.X, self.num_unit, self.activation)\n",
    "            \n",
    "            for idx in range(self.num_layer-2) :\n",
    "                norm = tf.contrib.layers.layer_norm(layer)\n",
    "                relu = tf.nn.relu(norm)\n",
    "                dense = tf.layers.dense(relu, self.num_unit)\n",
    "                layer = tf.layers.dropout(dense, training=self.training)\n",
    "                \n",
    "            layer = tf.layers.dense(layer, self.output_dim)\n",
    "            #########################\n",
    "            \n",
    "            \n",
    "            ## Classifier ##\n",
    "            self.logit = layer\n",
    "            self.softmax = tf.nn.softmax(self.logit)\n",
    "            self.softmax_logit = tf.nn.softmax_cross_entropy_with_logits(logits=self.logit, labels=self.Y)\n",
    "            ################\n",
    "            \n",
    "            \n",
    "            ## Learning ##\n",
    "            self.cost = tf.reduce_sum(self.softmax_logit)\n",
    "            \n",
    "            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope=self.name)\n",
    "            with tf.control_dependencies(update_ops):\n",
    "                self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.cost)\n",
    "            \n",
    "            self.prediction = tf.equal(tf.argmax(self.logit, 1), tf.argmax(self.Y, 1))     \n",
    "            self.accuracy = tf.reduce_mean(tf.cast(self.prediction, tf.float32))    \n",
    "            ##############\n",
    "        \n",
    "        \n",
    "    def train(self, X_input, Y_input, learning_rate, training=True):\n",
    "        feed_dict = {self.X: X_input, self.Y: Y_input, self.learning_rate: learning_rate, self.training: training}\n",
    "        _, cost = self.sess.run([self.optimizer, self.cost], feed_dict=feed_dict)\n",
    "        \n",
    "        return _, cost\n",
    "    \n",
    "    def predict(self, X_input, training=False):\n",
    "        feed_dict = {self.X: X_input, self.training: training}\n",
    "        result = self.sess.run([self.logit], feed_dict=feed_dict)\n",
    "            \n",
    "        return result\n",
    "    \n",
    "    def evaluate(self, X_input, Y_input):\n",
    "        size = X_input.shape[0]\n",
    "            \n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "            \n",
    "        for idx in range(0, size, self.batch_size):\n",
    "            X_batch = X_input[idx:idx + batch_size]\n",
    "            Y_batch = Y_input[idx:idx + batch_size]\n",
    "            feed_dict = {self.X: X_batch, self.Y: Y_batch, self.training: False}\n",
    "                \n",
    "            loss = self.cost\n",
    "            accuracy = self.accuracy\n",
    "                \n",
    "            step_loss, step_acc = self.sess.run([loss, accuracy], feed_dict=feed_dict)\n",
    "                \n",
    "            total_loss += step_loss * X_batch.shape[0]\n",
    "            total_acc += step_acc * X_batch.shape[0]\n",
    "            \n",
    "        total_loss /= size\n",
    "        total_acc /= size\n",
    "            \n",
    "        return total_loss, total_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br><br></br><br></br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate1 = 0.02\n",
    "learning_rate2 = 0.01\n",
    "learning_rate3 = 0.005\n",
    "learning_rate4 = 0.001\n",
    "\n",
    "total_epoch = 100\n",
    "batch_size = 500\n",
    "input_dim = np.array(total_lst).shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "model_lst = []\n",
    "\n",
    "for num_layer in [4,6,8] :\n",
    "    for num_unit in [256, 512] :\n",
    "        for activation in [None, tf.nn.relu] :\n",
    "            print(idx)\n",
    "            sess = tf.Session()\n",
    "            model = MLP(sess, \"model{}\".format(idx))\n",
    "            model.build(batch_size, input_dim, 4, num_layer, num_unit, activation)\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            \n",
    "            model_lst.append(model)\n",
    "            idx +=1\n",
    "            \n",
    "tl_ta_vl_va_lst = [[[],[],[],[]]]*len(model_lst)\n",
    "print(\"Ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Learning Started!')\n",
    "print(\"\")\n",
    "\n",
    "# train my model\n",
    "for epoch in range(total_epoch):\n",
    "    avg_cost = [0]*len(model_lst)\n",
    "    total_batch = int(len(training_lst) / batch_size)\n",
    "    idx = 0\n",
    "    \n",
    "    if epoch == 0 :\n",
    "        learning_rate = learning_rate1\n",
    "    elif epoch == 10 :\n",
    "        learning_rate = learning_rate2\n",
    "    elif epoch == 20 :\n",
    "        learning_rate = learning_rate3\n",
    "    elif epoch == 80 :\n",
    "        learning_rate = learning_rate4\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = training_lst[idx:idx+batch_size],training_label[idx:idx+batch_size]\n",
    "        \n",
    "        for model_num, model in enumerate(model_lst) :\n",
    "            _, c = model.train(batch_xs, batch_ys, learning_rate)\n",
    "            avg_cost[model_num] += c / total_batch\n",
    "        \n",
    "        idx += batch_size\n",
    "        if i%10 == 0 :\n",
    "            print(\"log :\", i)\n",
    "            \n",
    "    #train/valid cost & acc\n",
    "    print(\"***epoch*** : \", epoch)\n",
    "    for model_num, model in enumerate(model_lst) :\n",
    "        train_cost, train_acc = model.evaluate(training_lst, training_label)\n",
    "        valid_cost, valid_acc = model.evaluate(valid_lst, valid_label)\n",
    "\n",
    "        tl_ta_vl_va_lst[model_num][0].append(train_cost)\n",
    "        tl_ta_vl_va_lst[model_num][1].append(train_acc)\n",
    "        tl_ta_vl_va_lst[model_num][2].append(valid_cost)\n",
    "        tl_ta_vl_va_lst[model_num][3].append(valid_acc)\n",
    "\n",
    "        print(\"-- train {:.5f}({:.1f}%), valid{:.5f}({:.1f}%)\".format(train_cost, train_acc*100, valid_cost, valid_acc*100))\n",
    "    \n",
    "    for model in model_lst :\n",
    "        print('Accuracy:', model.evaluate(valid_lst, valid_label)[1])\n",
    "    print(\" \")\n",
    "\n",
    "print(\"\")\n",
    "print('Learning Finished!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <br></br><br></br><br></br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for idx in range(len(model_lst)) :\n",
    "    plt.plot(tl_ta_vl_va_lst[idx][0], label='training'+str(idx))\n",
    "    plt.plot(tl_ta_vl_va_lst[idx][2], label='valid'+str(idx))\n",
    "    plt.title(\"model\"+str(idx))\n",
    "    plt.grid(\"on\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for idx in range(len(model_lst)) :\n",
    "    plt.plot(tl_ta_vl_va_lst[idx][1], label='training'+str(idx))\n",
    "    plt.plot(tl_ta_vl_va_lst[idx][3], label='valid'+str(idx))\n",
    "    plt.title(\"model\"+str(idx))\n",
    "    plt.grid(\"on\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(model_lst)) :\n",
    "    plt.plot(tl_ta_vl_va_lst[idx][0], label='training'+str(idx))\n",
    "    \n",
    "plt.grid(\"on\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for idx in range(len(model_lst)) :\n",
    "    plt.plot(tl_ta_vl_va_lst[idx][2], label='training'+str(idx))\n",
    "    \n",
    "plt.grid(\"on\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(model_lst)) :\n",
    "    plt.plot(tl_ta_vl_va_lst[idx][1], label='training'+str(idx))\n",
    "    \n",
    "plt.grid(\"on\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(model_lst)) :\n",
    "    plt.plot(tl_ta_vl_va_lst[idx][3], label='training'+str(idx))\n",
    "    \n",
    "plt.grid(\"on\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br><br></br><br></br>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.reset_default_graph() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "for idx, model in enumerate(model_lst) :\n",
    "    saver.save(model.sess, './model/MLP_default_model_{}'.format(idx))\n",
    "\n",
    "print(\"Saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br><br></br><br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def making_data(csv) :\n",
    "    activity = pd.read_csv(csv).drop(\"Unnamed: 0\", axis=1)\n",
    "    \n",
    "    activity = activity[activity[\"label\"] == \"empty\"]\n",
    "    activity = activity.drop(\"label\", axis=1)\n",
    "    \n",
    "    activity = activity.sort_values([\"acc_id\",\"wk\"])[['acc_id', 'wk', 'cnt_clear_bam', 'cnt_clear_inzone_light','cnt_clear_inzone_normal', \n",
    "                                                  'cnt_clear_inzone_skilled', 'cnt_clear_inzone_solo', 'cnt_clear_raid', 'cnt_clear_raid_light',\n",
    "                                                  'cnt_dt', 'cnt_enter_bam', 'cnt_enter_inzone_light', 'cnt_enter_inzone_normal', \n",
    "                                                  'cnt_enter_inzone_skilled', 'cnt_enter_inzone_solo', 'cnt_enter_raid', 'cnt_enter_raid_light',\n",
    "                                                  'cnt_use_buffitem', 'district_chat', 'duel_cnt', 'duel_win', 'faction_chat', 'game_combat_time', \n",
    "                                                  'gathering_cnt', 'get_money','guild_chat', 'item_hongmun', 'making_cnt', 'normal_chat', \n",
    "                                                  'npc_exp', 'npc_hongmun', 'party_chat', 'partybattle_cnt', 'partybattle_win', 'play_time', \n",
    "                                                  'quest_exp', 'quest_hongmun', 'whisper_chat','first_week', 'payment_amount']]\n",
    "    \n",
    "    activity1 = activity[activity[\"wk\"]==1].drop([\"acc_id\", \"wk\"], axis=1)\n",
    "    activity2 = activity[activity[\"wk\"]==2].drop([\"acc_id\", \"wk\"], axis=1)\n",
    "    activity3 = activity[activity[\"wk\"]==3].drop([\"acc_id\", \"wk\"], axis=1)\n",
    "    activity4 = activity[activity[\"wk\"]==4].drop([\"acc_id\", \"wk\"], axis=1)\n",
    "    activity5 = activity[activity[\"wk\"]==5].drop([\"acc_id\", \"wk\"], axis=1)\n",
    "    activity6 = activity[activity[\"wk\"]==6].drop([\"acc_id\", \"wk\"], axis=1)\n",
    "    activity7 = activity[activity[\"wk\"]==7].drop([\"acc_id\", \"wk\"], axis=1)\n",
    "    activity8 = activity[activity[\"wk\"]==8].drop([\"acc_id\", \"wk\"], axis=1)\n",
    "    \n",
    "    label = activity[\"acc_id\"].values\n",
    "    activity = np.concatenate([activity1.values, activity2.values, activity3.values, activity4.values,\n",
    "                               activity5.values, activity6.values, activity7.values, activity8.values], axis=1)\n",
    "\n",
    "    total_lst = activity\n",
    "    return total_lst, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data, test_acc_id = making_data(\"OnlyExpanded.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br><br></br><br></br> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for model in model_lst :\n",
    "    result.append(np.argmax(model.predict(test_data), axis=2)) \n",
    "    \n",
    "result = list(map(lambda x : x.tolist()[0], result))\n",
    "for r_lst in result :\n",
    "    print(\"week: {}, month: {}, 2month: {}, retained: {}\".format(r_lst.count(0), r_lst.count(1), r_lst.count(2), r_lst.count(3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_df = pd.DataFrame(sorted(list(set(list(test_acc_id))))).rename(columns = {0 : \"acc_id\"})\n",
    "result_df = pd.DataFrame(result).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df2 = pd.concat([label_df, result_df], axis=1)\n",
    "result_df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
