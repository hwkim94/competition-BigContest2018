{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN() :\n",
    "    def __init__(self, sess, name):\n",
    "        self.sess = sess\n",
    "        self.name = name\n",
    "        \n",
    "    def build(self, batch_size, length, dim, is_embedding, emb_width, num_unit, is_fc, fc_num_unit, fc_activation, cost_function, output_dim) :\n",
    "        with tf.variable_scope(self.name) :\n",
    "            \n",
    "            ## Setting ##\n",
    "            self.batch_size = batch_size\n",
    "            self.length = length\n",
    "            self.dim = dim\n",
    "            self.is_embedding = is_embedding\n",
    "            self.emb_width = emb_width\n",
    "            self.num_unit = num_unit\n",
    "            self.is_fc = is_fc\n",
    "            self.fc_num_unit = fc_num_unit\n",
    "            self.fc_activation = fc_activation\n",
    "            self.output_dim = output_dim\n",
    "            \n",
    "            self.X = tf.placeholder(tf.float32, [self.batch_size, self.length, self.dim])\n",
    "            self.Y = tf.placeholder(tf.float32, [self.batch_size, self.output_dim])\n",
    "            self.learning_rate =  tf.placeholder(tf.float32)\n",
    "            self.training = tf.placeholder(tf.bool)\n",
    "            #############\n",
    "            \n",
    "            \n",
    "            ## Embedding ##\n",
    "            if self.is_embedding :\n",
    "                W_emb = tf.Variable(tf.random_normal([self.width, self.emb_width]))\n",
    "                self.X = tf.concat(self.X, tf.matmul(self.X, W_emb), axis=2)\n",
    "            ###############\n",
    "            \n",
    "            \n",
    "            ## RNN ##\n",
    "            f_cell1 = tf.nn.rnn_cell.BasicLSTMCell(self.num_unit)\n",
    "            f_cell2 = tf.nn.rnn_cell.BasicLSTMCell(self.num_unit)\n",
    "            f_multi_cell = tf.nn.rnn_cell.MultiRNNCell([f_cell1, f_cell2])\n",
    "            \n",
    "            b_cell1 = tf.nn.rnn_cell.BasicLSTMCell(self.num_unit)\n",
    "            b_cell2 = tf.nn.rnn_cell.BasicLSTMCell(self.num_unit)\n",
    "            b_multi_cell = tf.nn.rnn_cell.MultiRNNCell([b_cell1, b_cell2])\n",
    "            \n",
    "            \n",
    "            f_output, f_state = tf.nn.dynamic_rnn(f_multi_cell, self.X, dtype=tf.float32,  scope=\"forward\")\n",
    "            b_output, b_state = tf.nn.dynamic_rnn(b_multi_cell, tf.reverse(self.X, axis=[1]), dtype=tf.float32, scope=\"backward\")\n",
    "            \n",
    "            f_output = tf.transpose(f_output,[1,0,2])[-1]\n",
    "            b_output = tf.transpose(b_output,[1,0,2])[-1]\n",
    "            \n",
    "            rnn_result = tf.concat([f_output, b_output], axis=1)\n",
    "            #########\n",
    "            \n",
    "            \n",
    "            ## Classifier ##\n",
    "            if is_fc : \n",
    "                dense= tf.layers.dense(rnn_result, self.fc_num_unit)\n",
    "                norm = tf.contrib.layers.layer_norm(dense)\n",
    "                relu = tf.nn.relu(norm)\n",
    "                self.logit = tf.layers.dense(norm, 4)\n",
    "            else :\n",
    "                self.logit = tf.layers.dense(rnn_result, 4)\n",
    "                \n",
    "            self.softmax = tf.nn.softmax(self.logit)\n",
    "            ################\n",
    "            \n",
    "            \n",
    "            ## Learning ##\n",
    "            if cost_function == \"f1\" :\n",
    "                self.numerator = tf.reduce_sum(self.softmax*self.Y)\n",
    "                self.denominator = tf.reduce_sum(self.softmax*self.Y + self.Y)\n",
    "                self.cost = -2 * self.numerator / self.denominator\n",
    "                \n",
    "            else :\n",
    "                self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=self.logit, labels=self.Y))\n",
    "\n",
    "            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope=self.name)\n",
    "            with tf.control_dependencies(update_ops):\n",
    "                self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.cost)\n",
    "            \n",
    "            self.prediction = tf.equal(tf.argmax(self.logit, 1), tf.argmax(self.Y, 1))     \n",
    "            self.accuracy = tf.reduce_mean(tf.cast(self.prediction, tf.float32))    \n",
    "            ##############\n",
    "        \n",
    "        \n",
    "    def train(self, X_input, Y_input, learning_rate, training=True):\n",
    "        feed_dict = {self.X: X_input, self.Y: Y_input, self.learning_rate: learning_rate, self.training: training}\n",
    "        _, cost = self.sess.run([self.optimizer, self.cost], feed_dict=feed_dict)\n",
    "        \n",
    "        return _, cost\n",
    "    \n",
    "    def predict(self, X_input, training=False):\n",
    "        feed_dict = {self.X: X_input, self.training: training}\n",
    "        result = self.sess.run([self.logit], feed_dict=feed_dict)\n",
    "            \n",
    "        return result\n",
    "    \n",
    "    def evaluate(self, X_input, Y_input):\n",
    "        size = X_input.shape[0]\n",
    "            \n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "            \n",
    "        for idx in range(0, size, self.batch_size):\n",
    "            X_batch = X_input[idx:idx + batch_size]\n",
    "            Y_batch = Y_input[idx:idx + batch_size]\n",
    "            feed_dict = {self.X: X_batch, self.Y: Y_batch, self.training: False}\n",
    "                \n",
    "            loss = self.cost\n",
    "            accuracy = self.accuracy\n",
    "                \n",
    "            step_loss, step_acc = self.sess.run([loss, accuracy], feed_dict=feed_dict)\n",
    "                \n",
    "            total_loss += step_loss * X_batch.shape[0]\n",
    "            total_acc += step_acc * X_batch.shape[0]\n",
    "            \n",
    "        total_loss /= size\n",
    "        total_acc /= size\n",
    "            \n",
    "        return total_loss, total_acc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
