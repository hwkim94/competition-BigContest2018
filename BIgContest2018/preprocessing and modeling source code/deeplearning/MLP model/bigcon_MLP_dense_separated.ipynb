{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP() :\n",
    "    def __init__(self, sess, name):\n",
    "        self.sess = sess\n",
    "        self.name = name\n",
    "        \n",
    "    def day_MLP(self, X_input) :\n",
    "        layer = tf.layers.dense(X_input, self.emb_dim)\n",
    "        \n",
    "        for idx in range(self.num_layer//3+1) :\n",
    "            norm1 = tf.contrib.layers.layer_norm(layer)\n",
    "            relu1 = tf.nn.leaky_relu(norm1)\n",
    "            layer1 = tf.layers.dense(relu1, self.emb_dim*2)\n",
    "                \n",
    "            norm2 = tf.contrib.layers.layer_norm(layer1)\n",
    "            relu2 = tf.nn.leaky_relu(norm2)\n",
    "            layer2 = tf.layers.dense(relu2, self.emb_dim)\n",
    "            \n",
    "            layer = layer2 + layer\n",
    "                \n",
    "        summary_layer = tf.layers.dense(layer, self.emb_dim//2)\n",
    "        return summary_layer\n",
    "        \n",
    "    \n",
    "    def build(self, batch_size, input_dim, emb_dim, output_dim, num_layer, num_unit, activation) :\n",
    "        with tf.variable_scope(self.name) :\n",
    "            ## Setting ##\n",
    "            # input  : ? x input_length x input_dim\n",
    "            self.X = tf.placeholder(tf.float32, [None, input_dim])\n",
    "            self.Y = tf.placeholder(tf.float32, [None, output_dim])\n",
    "            self.learning_rate =  tf.placeholder(tf.float32)\n",
    "            self.training = tf.placeholder(tf.bool)\n",
    "            \n",
    "            self.batch_size = batch_size\n",
    "            self.day_dim = input_dim//8\n",
    "            self.emb_dim = emb_dim\n",
    "            self.output_dim = output_dim\n",
    "            self.num_layer = num_layer\n",
    "            self.num_unit = num_unit\n",
    "            self.activation = activation\n",
    "            #############\n",
    "            \n",
    "            \n",
    "            ## MLP ##\n",
    "            day1 = self.day_MLP(self.X[:,:self.day_dim])\n",
    "            day2 = self.day_MLP(self.X[:,self.day_dim : self.day_dim*2])\n",
    "            day3 = self.day_MLP(self.X[:,self.day_dim*2 : self.day_dim*3])\n",
    "            day4 = self.day_MLP(self.X[:,self.day_dim*3 : self.day_dim*4])\n",
    "            day5 = self.day_MLP(self.X[:,self.day_dim*4 : self.day_dim*5])\n",
    "            day6 = self.day_MLP(self.X[:,self.day_dim*5 : self.day_dim*6])\n",
    "            day7 = self.day_MLP(self.X[:,self.day_dim*6 : self.day_dim*7])\n",
    "            day8 = self.day_MLP(self.X[:,self.day_dim*7 :])\n",
    "            \n",
    "            day_total = tf.concat([day1,day2,day3,day4,day5,day6,day7,day8], axis=1)\n",
    "            day_norm = tf.contrib.layers.layer_norm(day_total)\n",
    "            day_relu = tf.nn.leaky_relu(day_norm)\n",
    "            layer = tf.layers.dense(day_relu, self.num_unit)\n",
    "            \n",
    "            layer_lst = []\n",
    "            layer_lst.append(layer)\n",
    "            \n",
    "            for idx in range((self.num_layer-2)//2) :\n",
    "                dense_layer = tf.concat(layer_lst, axis=1)\n",
    "                \n",
    "                norm1 = tf.contrib.layers.layer_norm(dense_layer)\n",
    "                relu1 = tf.nn.leaky_relu(norm1)\n",
    "                layer1 = tf.layers.dense(relu1, self.num_unit*2)\n",
    "                \n",
    "                norm2 = tf.contrib.layers.layer_norm(layer1)\n",
    "                relu2 = tf.nn.leaky_relu(norm2)\n",
    "                layer2 = tf.layers.dense(relu2, self.num_unit)\n",
    "                dropout = tf.layers.dropout(layer2, training=self.training)\n",
    "                \n",
    "                layer_lst.append(dropout)\n",
    "                \n",
    "            layer_lst2 = []\n",
    "            concat_layer = tf.concat(layer_lst, axis=1)\n",
    "            batch, hidden = concat_layer.get_shape().as_list()\n",
    "            \n",
    "            compression_norm = tf.contrib.layers.layer_norm(concat_layer)\n",
    "            compression_relu = tf.nn.leaky_relu(compression_norm)\n",
    "            compression_layer = tf.layers.dense(compression_relu, hidden//2)\n",
    "            layer_lst2.append(compression_layer)\n",
    "            \n",
    "            for idx in range((self.num_layer-2)//2) :\n",
    "                dense_layer2 = tf.concat(layer_lst2, axis=1)\n",
    "                \n",
    "                norm1 = tf.contrib.layers.layer_norm(dense_layer2)\n",
    "                relu1 = tf.nn.leaky_relu(norm1)\n",
    "                layer1 = tf.layers.dense(relu1, self.num_unit*2)\n",
    "                \n",
    "                norm2 = tf.contrib.layers.layer_norm(layer1)\n",
    "                relu2 = tf.nn.leaky_relu(norm2)\n",
    "                layer2 = tf.layers.dense(relu2, self.num_unit)\n",
    "                dropout = tf.layers.dropout(layer2, training=self.training)\n",
    "                \n",
    "                layer_lst2.append(dropout)\n",
    "            \n",
    "            concat_layer2 = tf.concat(layer_lst2, axis=1)\n",
    "            compression_norm2 =tf.contrib.layers.layer_norm(concat_layer2)\n",
    "            compression_relu2 = tf.nn.leaky_relu(compression_norm2)\n",
    "            compression_layer2 = tf.layers.dense(compression_relu2, self.num_unit)\n",
    "            layer = tf.layers.dense(compression_layer2, self.output_dim)\n",
    "            #########################\n",
    "            \n",
    "            \n",
    "            ## Classifier ##\n",
    "            self.logit = layer\n",
    "            self.softmax = tf.nn.softmax(self.logit)\n",
    "            self.softmax_logit = tf.nn.softmax_cross_entropy_with_logits(logits=self.logit, labels=self.Y)\n",
    "            ################\n",
    "            \n",
    "            \n",
    "            ## Learning ##\n",
    "            self.cost =  tf.reduce_mean(self.softmax_logit)\n",
    "\n",
    "            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope=self.name)\n",
    "            with tf.control_dependencies(update_ops):\n",
    "                self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.cost)\n",
    "            \n",
    "            self.prediction = tf.equal(tf.argmax(self.logit, 1), tf.argmax(self.Y, 1))     \n",
    "            self.accuracy = tf.reduce_mean(tf.cast(self.prediction, tf.float32))    \n",
    "            ##############\n",
    "        \n",
    "        \n",
    "    def train(self, X_input, Y_input, learning_rate, training=True):\n",
    "        feed_dict = {self.X: X_input, self.Y: Y_input, self.learning_rate: learning_rate, self.training: training}\n",
    "        _, cost = self.sess.run([self.optimizer, self.cost], feed_dict=feed_dict)\n",
    "        \n",
    "        return _, cost\n",
    "    \n",
    "    def predict(self, X_input, training=False):\n",
    "        feed_dict = {self.X: X_input, self.training: training}\n",
    "        result = self.sess.run([self.logit], feed_dict=feed_dict)\n",
    "            \n",
    "        return result\n",
    "    \n",
    "    def evaluate(self, X_input, Y_input):\n",
    "        size = X_input.shape[0]\n",
    "            \n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "            \n",
    "        for idx in range(0, size, self.batch_size):\n",
    "            X_batch = X_input[idx:idx + batch_size]\n",
    "            Y_batch = Y_input[idx:idx + batch_size]\n",
    "            feed_dict = {self.X: X_batch, self.Y: Y_batch, self.training: False}\n",
    "                \n",
    "            loss = self.cost\n",
    "            accuracy = self.accuracy\n",
    "                \n",
    "            step_loss, step_acc = self.sess.run([loss, accuracy], feed_dict=feed_dict)\n",
    "                \n",
    "            total_loss += step_loss * X_batch.shape[0]\n",
    "            total_acc += step_acc * X_batch.shape[0]\n",
    "            \n",
    "        total_loss /= size\n",
    "        total_acc /= size\n",
    "            \n",
    "        return total_loss, total_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
